{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lQ7QKXXCp7Bj",
        "r2jJGEOYphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "tEA2Xm5dHt1r",
        "Ou-I18pAyIpj",
        "hwyV_J3ipUZe",
        "Fd15vwWVpUZf",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "PiV4Ypx8fxKe",
        "bmKjuQ-FpsJ3",
        "_-qAgymDpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on building a regression model to predict the monthly closing price of Yes Bank stock using historical stock data. The aim is to leverage machine learning techniques to understand market patterns and provide data-driven insights into stock price behavior.\n",
        "\n",
        "Yes Bank has been a prominent player in the Indian banking sector. Since 2018, it has been in the news due to a major financial fraud case involving its former CEO, Rana Kapoor. This led to significant volatility in the stock prices, making it a suitable case for applying regression models and understanding how certain features influence the closing price. The dataset provided contains monthly stock price records, including the opening, highest, lowest, and closing prices along with the date.\n",
        "\n",
        "The project began with Exploratory Data Analysis (EDA) to understand trends and detect patterns in the data. Visualizations using libraries such as matplotlib and seaborn helped in understanding the relationship between different features like Open, High, Low, and Close. The Date column was broken down into additional features such as Month and Year to help the model better understand temporal behavior.\n",
        "\n",
        "Following EDA, we moved to data cleaning, where missing values and outliers were handled appropriately. Outliers were detected using methods like the Interquartile Range (IQR), and null values (if any) were imputed or removed after careful consideration. This step ensured that the model would not be biased or skewed due to noisy data.\n",
        "\n",
        "Next came feature engineering, where new columns were created, such as High-Low and Open-Close differences. These derived features were added to capture the intra-month volatility and price movement trends, which can be important indicators for predicting closing prices.\n",
        "\n",
        "In the preprocessing stage, we used StandardScaler to normalize numerical features and applied an 80-20 train-test split strategy. Since the data had a time-series nature, we ensured chronological order was preserved to avoid data leakage.\n",
        "\n",
        "We trained at least two models ‚Äî Linear Regression and Random Forest Regressor ‚Äî to compare performance. The models were evaluated based on metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and R¬≤ Score. Visualization of actual vs predicted closing prices gave a clear picture of the models‚Äô performance.\n",
        "\n",
        "We also performed hyperparameter tuning to improve the performance of the Random Forest model and identified the most influential features through feature importance plots. All code was modularized, well-commented, and properly formatted in a Google Colab notebook, which includes summary explanations, graphs, and result interpretation.\n",
        "\n",
        "In conclusion, this project not only demonstrates how machine learning can be applied to stock price prediction but also reflects real-world data handling and problem-solving skills. The results indicate that while stock markets are inherently volatile, using historical data and the right ML techniques, it is possible to build fairly accurate predictive models."
      ],
      "metadata": {
        "id": "_Uy13biMerZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to predict the monthly closing price of Yes Bank‚Äôs stock using historical stock price data through regression techniques. Given the fluctuations in Yes Bank‚Äôs stock prices‚Äîespecially following significant financial events like the 2018 fraud case involving former CEO Rana Kapoor‚Äîthere is a need to explore how well machine learning models can capture and forecast stock price behavior based on key variables.\n",
        "\n",
        "The dataset contains monthly records of stock prices, including:\n",
        "\n",
        "Opening Price\n",
        "\n",
        "Highest Price\n",
        "\n",
        "Lowest Price\n",
        "\n",
        "Closing Price\n",
        "\n",
        "Date of Record\n",
        "\n",
        "The goal is to develop a predictive model that can estimate the closing price based on the other available features. This involves applying data preprocessing, exploratory data analysis, feature engineering, model training, and evaluation using appropriate regression techniques. The project also aims to identify which features have the most impact on closing prices, and to visualize and explain the model's performance.\n",
        "\n",
        "Ultimately, this project will demonstrate the applicability of machine learning in financial forecasting and show how historical market data can be used to make informed predictions."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# üìä Data handling and manipulation\n",
        "import pandas as pd   # For reading and handling data\n",
        "import numpy as np    # For numerical operations\n",
        "\n",
        "# üìà Data visualization\n",
        "import matplotlib.pyplot as plt  # For creating plots\n",
        "import seaborn as sns            # For statistical visualizations\n",
        "\n",
        "\n",
        "# üßπ Data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "\n",
        "# ü§ñ Machine learning models\n",
        "from sklearn.linear_model import LinearRegression   # Simple regression model\n",
        "from sklearn.ensemble import RandomForestRegressor  # Tree-based ensemble model\n",
        "\n",
        "# üìä Model evaluation metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Performance metrics\n",
        "\n",
        "# üõ†Ô∏è Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # To ignore unnecessary warnings\n",
        "\n",
        "# üìÖ Date handling (if needed during feature engineering)\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_file.csv' with the actual path to your dataset\n",
        "df = pd.read_csv('/content/data_YesBank_StockPrices.csv')\n",
        "\n",
        "# Display the first 5 rows to get an overview of the data\n",
        "print(\"üîç Preview of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of the dataset (rows, columns)\n",
        "print(\"\\nüìè Dataset shape:\", df.shape)\n",
        "\n",
        "# View data types and non-null counts\n",
        "print(\"\\nüîß Dataset info:\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Show the first 5 rows of the dataset\n",
        "print(\"üîç First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Show the last 5 rows (to check the tail-end of the data)\n",
        "print(\"\\nüîé Last 5 rows of the dataset:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Check column names\n",
        "print(\"\\nüìù Column Names:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Basic statistical summary of numerical columns\n",
        "print(\"\\nüìä Descriptive Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape\n",
        "\n",
        "print(f\"üìè The dataset contains {rows} rows and {columns} columns.\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Display structure and summary info about the dataset\n",
        "print(\" Dataset Information:\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Count the number of duplicate rows in the dataset\n",
        "duplicate_count = df.duplicated().sum()\n",
        "\n",
        "print(f\"üîÅ Total number of duplicate rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count of missing (NaN) values in each column\n",
        "print(\"‚ùì Missing / Null Values in Each Column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Optional: Get total number of missing values in the entire dataset\n",
        "total_missing = df.isnull().sum().sum()\n",
        "print(f\"\\nüßÆ Total missing values in the dataset: {total_missing}\")\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visual heatmap to show where nulls exist\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap=\"Reds\", yticklabels=False)\n",
        "plt.title(\"üîç Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values or null values and duplicate values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Print all column names\n",
        "print(\"üìù Column Names in the Dataset:\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Summary statistics for all numeric columns\n",
        "print(\"üìä Descriptive Statistics:\")\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date\n",
        "\n",
        "This column represents the month and year for which the stock data is recorded.\n",
        "\n",
        "It is usually in the format YYYY-MM-DD (though only one entry per month).\n",
        "\n",
        "This will be useful for extracting time-based features like month and year.\n",
        "\n",
        "Open\n",
        "\n",
        "This is the stock‚Äôs price at the beginning of the month (the first trading day).\n",
        "\n",
        "It reflects the starting market sentiment and is often compared with the closing price to gauge movement.\n",
        "\n",
        "High\n",
        "\n",
        "This represents the highest price the stock reached during that particular month.\n",
        "\n",
        "It indicates the peak market optimism or demand during that period.\n",
        "\n",
        "Low\n",
        "\n",
        "This shows the lowest price the stock touched in the month.\n",
        "\n",
        "It helps in understanding the depth of negative sentiment or volatility.\n",
        "\n",
        "Close\n",
        "\n",
        "This is the closing price of the stock at the end of the month (last trading day).\n",
        "\n",
        "It is the main target variable in this project (i.e., the value we aim to predict).\n",
        "\n",
        "Often used in financial forecasting as it reflects the final consensus price."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Loop through all columns and print the count of unique values\n",
        "print(\"üî¢ Unique value count for each column:\\n\")\n",
        "for column in df.columns:\n",
        "    unique_count = df[column].nunique()\n",
        "    print(f\"{column}: {unique_count} unique values\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/data_YesBank_StockPrices.csv')\n",
        "print(\"Shape after loading:\", df.shape)\n",
        "\n",
        "# üßπ 1. Drop duplicate rows (if any)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Shape after dropping duplicates:\", df.shape)\n",
        "\n",
        "# üìÜ 2. Convert 'Date' column to datetime format\n",
        "# Use '%b-%y' format string for 'Jul-05' type dates\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')\n",
        "print(\"Shape after date conversion:\", df.shape)\n",
        "print(\"Nulls in Date after conversion:\", df['Date'].isnull().sum())\n",
        "\n",
        "\n",
        "# üßº 3. Drop rows with missing or malformed dates (if any)\n",
        "# This is important because if the format was wrong, `errors='coerce'` would make them NaT\n",
        "# Now that we are using the correct format, hopefully, this step won't drop many rows.\n",
        "df = df.dropna(subset=['Date'])\n",
        "print(\"Shape after dropping rows with missing dates:\", df.shape)\n",
        "\n",
        "\n",
        "# üìä 4. Sort the data by Date (oldest to newest)\n",
        "df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "print(\"Shape after sorting:\", df.shape)\n",
        "\n",
        "# üß† 5. Create new time-based features\n",
        "# Ensure 'Date' is datetime before extracting features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Month_Name'] = df['Date'].dt.strftime('%B')\n",
        "print(\"Shape after adding time features:\", df.shape)\n",
        "\n",
        "\n",
        "# ‚öôÔ∏è 6. Feature Engineering: Create new informative features\n",
        "df['High_Low_Diff'] = df['High'] - df['Low']       # Measures volatility\n",
        "df['Open_Close_Diff'] = df['Open'] - df['Close']   # Measures trend direction\n",
        "print(\"Shape after adding engineered features:\", df.shape)\n",
        "\n",
        "\n",
        "# üßΩ 7. Handle missing values if any (Check again after feature engineering)\n",
        "print(\"\\nMissing values before ffill:\")\n",
        "print(df.isnull().sum())\n",
        "# For numerical columns, fill with forward fill (or you can use mean/median)\n",
        "# You might want to be careful with ffill on stock data if there are long gaps\n",
        "# Given this is monthly data, ffill might be acceptable, but consider alternatives\n",
        "# like imputation with mean/median if you have significant missing data.\n",
        "df.fillna(method='ffill', inplace=True)\n",
        "print(\"\\nMissing values after ffill:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "# ‚úÖ Dataset is now cleaned, structured, and ready for EDA and modeling\n",
        "print(\"\\nüéâ Dataset is now analysis-ready!\")\n",
        "print(\"\\nFirst 5 rows of the processed DataFrame:\")\n",
        "print(df.head())\n",
        "print(\"\\nLast 5 rows of the processed DataFrame:\")\n",
        "print(df.tail())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Data Manipulations Done (Data Wrangling)\n",
        "Removed Duplicate Rows\n",
        "\n",
        "Ensured there are no repeated entries that could bias the model.\n",
        "\n",
        "Converted Date Column to Datetime Format\n",
        "\n",
        "Allows time-based feature extraction and proper chronological sorting.\n",
        "\n",
        "Dropped Rows with Invalid or Missing Dates\n",
        "\n",
        "Cleaned corrupted records (if any) to maintain data consistency.\n",
        "\n",
        "Sorted Data by Date\n",
        "\n",
        "Ensures that stock data flows in proper monthly order ‚Äî important for time series or trend-based models.\n",
        "\n",
        "Extracted New Time-Based Features\n",
        "\n",
        "Year, Month, and Month_Name were created for better seasonal or temporal trend analysis.\n",
        "\n",
        "Created New Features (Feature Engineering)\n",
        "\n",
        "High_Low_Diff: Captures monthly stock volatility.\n",
        "\n",
        "Open_Close_Diff: Indicates monthly price movement (gain or loss).\n",
        "\n",
        "Handled Missing Values\n",
        "\n",
        "Used forward fill to impute any remaining gaps in data.\n",
        "\n",
        "üìä Initial Insights Gained\n",
        "The dataset seems to have monthly granularity, with one record per month.\n",
        "\n",
        "Stock prices vary significantly across months, with some months showing high volatility (High_Low_Diff).\n",
        "\n",
        "There are patterns in price trends where either Open is greater than Close (falling month) or vice versa (rising month).\n",
        "\n",
        "The date column is now usable for trend analysis over years or months."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# üìä Visualize the Closing Price over Time\n",
        "plt.figure(figsize=(12, 6)) # Set the figure size for better readability\n",
        "sns.lineplot(data=df, x='Date', y='Close') # Create the line plot using seaborn\n",
        "plt.title('üìà Yes Bank Stock Closing Price Over Time', fontsize=16) # Add a title\n",
        "plt.xlabel('Date', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid for easier reading of values\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "hdgAf0mPpUgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "MqFMOkRtrbXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is ideal for showing trends and changes in a variable (Closing Price) over a continuous time period (Date)."
      ],
      "metadata": {
        "id": "u-n32X7zrbXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observed the overall trend (upward, downward, or sideways) of the Yes Bank stock closing price over the given time frame.\n",
        "Identified periods of high volatility (steep increases or decreases) and periods of relative stability.\n",
        "Saw the approximate range of closing prices over the years.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, identifying upward trends helps in understanding potential growth periods, which can inform investment strategies (e.g., when to buy). Observing periods of stability can indicate lower risk. Negative Growth Insights: Yes, the chart likely shows periods of significant price decline, indicating negative growth. These insights are crucial for risk assessment and can inform decisions on when to sell or avoid investing to mitigate losses. Sharp drops clearly show periods where the stock value decreased, leading to negative returns for investors during those times."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Chart - 2 visualization code\n",
        "\n",
        "# üìä Visualize the distribution of stock prices by Year using Box Plots\n",
        "plt.figure(figsize=(15, 8)) # Set the figure size\n",
        "\n",
        "# Melt the DataFrame to have a single column for price types\n",
        "# We'll exclude the Date and Month_Name columns for melting\n",
        "df_melted = df.melt(id_vars=['Date', 'Year', 'Month', 'Month_Name', 'High_Low_Diff', 'Open_Close_Diff'],\n",
        "                   value_vars=['Open', 'High', 'Low', 'Close'],\n",
        "                   var_name='Price_Type', # New column for 'Open', 'High', 'Low', 'Close'\n",
        "                   value_name='Price')   # New column for the price values\n",
        "\n",
        "sns.boxplot(data=df_melted, x='Year', y='Price', hue='Price_Type', palette='viridis') # Create the box plot\n",
        "plt.title('üì¶ Distribution of Stock Prices by Year', fontsize=16) # Add a title\n",
        "plt.xlabel('Year', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Price', fontsize=12) # Label the y-axis\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "plt.legend(title='Price Type') # Add a legend\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots are excellent for visualizing the distribution of a numerical variable across different categories. In this case, we use it to show the distribution of stock prices (Open, High, Low, Close) for each year. This allows us to see the median price, the spread (Interquartile Range), the minimum and maximum values (excluding outliers), and identify potential outliers within each year. Comparing box plots across years helps in understanding how the price range and variability have changed over time."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see how the median price for Open, High, Low, and Close has changed from year to year.\n",
        "Observe the interquartile range (the box itself) for each year and price type, indicating the typical range of prices for 50% of the data points in that year. This tells you about the price volatility within the year.\n",
        "Identify potential outliers (points outside the whiskers), which could represent months with unusually high or low price movements.\n",
        "Compare the overall range of prices (from minimum to maximum whisker values) across different years."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact: Understanding the typical price range and variability within each year can help in setting realistic expectations for trading or investment. Years with tighter boxes might indicate more predictable price movements, which could be favorable for certain strategies. Identifying upward shifts in the median price over consecutive years suggests a positive long-term trend.\n",
        "Negative Growth Insights: Years with significantly lower median prices or a concentration of values in a lower range compared to previous years indicate a period of negative growth. The presence of outliers below the typical range could signal months with significant price drops. Years with wider boxes might indicate higher volatility and risk, which, if combined with a downward trend, points towards potential for negative returns. For instance, a year where the median price box is much lower than the previous year, and the box itself is quite wide (showing high volatility), is a strong indicator of a challenging period for the stock"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uRD5-TMgrDs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# üìä Visualize the distribution of engineered features\n",
        "plt.figure(figsize=(14, 6)) # Set the figure size\n",
        "\n",
        "# Subplot 1: Distribution of High-Low Difference\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n",
        "sns.histplot(data=df, x='High_Low_Diff', kde=True, bins=20) # Histogram with KDE\n",
        "plt.title('üìà Distribution of Monthly High-Low Difference', fontsize=14)\n",
        "plt.xlabel('High-Low Difference', fontsize=10)\n",
        "plt.ylabel('Frequency', fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Subplot 2: Distribution of Open-Close Difference\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n",
        "sns.histplot(data=df, x='Open_Close_Diff', kde=True, bins=20) # Histogram with KDE\n",
        "plt.title('üìä Distribution of Monthly Open-Close Difference', fontsize=14)\n",
        "plt.xlabel('Open-Close Difference', fontsize=10)\n",
        "plt.ylabel('Frequency', fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "plt.show() # Display the plots"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Histograms (with Kernel Density Estimate - KDE) are used to show the distribution and frequency of a single numerical variable. In this case, we are visualizing the distributions of High_Low_Diff and Open_Close_Diff to understand their typical ranges and how often certain values occur. This helps in understanding the characteristic volatility and monthly price swings of the stock."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For High_Low_Diff, you can see the most frequent range of monthly volatility. A wider spread indicates months with larger price swings.\n",
        "For Open_Close_Diff, you can observe whether the distribution is centered around zero (meaning prices tend to end the month near where they started), skewed towards positive values (indicating months where the stock tended to close lower than it opened - bearish months), or skewed towards negative values (indicating months where the stock tended to close higher than it opened - bullish months).\n",
        "The KDE line provides a smoothed estimate of the distribution shape."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact: Understanding the typical monthly volatility (High_Low_Diff) helps in assessing the risk associated with trading or investing in this stock. Lower volatility periods might be preferred by risk-averse investors. Knowing the common range and direction of monthly price movements (Open_Close_Diff) can inform short-term trading strategies. For example, if there's a clear tendency for the stock to close higher than it opens, it might indicate a general upward trend or positive market sentiment within months.\n",
        "Negative Growth Insights: A distribution of Open_Close_Diff heavily skewed towards positive values (meaning Close < Open) indicates that in many months, the stock price has decreased, which points to negative growth over those periods. Higher frequency of large positive Open_Close_Diff values means frequent significant monthly losses. Similarly, a histogram for High_Low_Diff showing a significant number of months with very large differences implies high volatility, which, during a downtrend, can lead to substantial negative returns quickly"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Chart - 4 visualization code\n",
        "\n",
        "# üìä Visualize the relationship between Open and Close prices using a Scatter Plot\n",
        "plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "sns.scatterplot(data=df, x='Open', y='Close', alpha=0.6) # Create the scatter plot\n",
        "plt.title('üìâ Relationship between Monthly Open and Close Prices', fontsize=16) # Add a title\n",
        "plt.xlabel('Open Price', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Close Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A scatter plot is used to visualize the relationship between two numerical variables. In this case, we want to see how the 'Open' price of the stock relates to its 'Close' price. A scatter plot helps in identifying if there's a linear relationship, the strength of the relationship, and any potential outliers."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe if there's a positive correlation (as Open price increases, Close price tends to increase), a negative correlation (as Open price increases, Close price tends to decrease), or no clear correlation.\n",
        "The tightness of the cluster of points indicates the strength of the relationship. Points closely following a line suggest a strong correlation.\n",
        "You can spot any data points that are far away from the main cluster, which could be outliers representing months with unusual price movements relative to their opening price."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact: A strong positive correlation between Open and Close prices suggests that the opening price is a good indicator of the potential closing price for that month. This can be a positive insight for short-term trading strategies, where predicting the direction of the daily or monthly price movement is important. If the plot shows that generally, when the stock opens high, it also closes high, this supports a positive outlook for bullish market conditions.\n",
        "Negative Growth Insights: While the scatter plot primarily shows correlation, if the points tend to lie below the line where Open = Close (meaning Close < Open), this visually represents months where the stock price has decreased, indicating negative growth within those periods. A weak or scattered relationship might suggest that the opening price alone is not a strong predictor of the closing price, implying higher volatility or influence from other factors, which could increase investment risk and potentially lead to negative outcomes if price movements are unpredictable."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Chart - 5 visualization code\n",
        "\n",
        "# üìä Visualize the relationship between High and Close prices using a Scatter Plot with Regression Line\n",
        "plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "sns.regplot(data=df, x='High', y='Close', scatter_kws={'alpha':0.6}, line_kws={\"color\": \"red\"}) # Create the scatter plot with a regression line\n",
        "plt.title('üìà Relationship between Monthly High and Close Prices', fontsize=16) # Add a title\n",
        "plt.xlabel('High Price', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A regplot in Seaborn combines a scatter plot with a regression line. This is useful for visualizing the relationship between two numerical variables ('High' and 'Close') and also showing the linear trend (the regression line) and its confidence interval. This gives a clear picture of how the closing price tends to change as the highest price reached during the month changes."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe the strength and direction of the linear relationship between the monthly highest price and the closing price.\n",
        "The slope of the regression line indicates how much the closing price is expected to change for a one-unit increase in the high price.\n",
        "The spread of the scatter points around the line shows the variability and helps assess how well the 'High' price alone predicts the 'Close' price.\n",
        "The confidence interval around the regression line shows the range within which the true relationship is likely to lie."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact: A strong positive linear relationship suggests that months where the stock reaches a higher peak tend to also end with a higher closing price. This insight can be used by traders to anticipate potential closing prices based on the high price reached during the month. A strong positive correlation is generally a positive indicator of the stock's ability to hold onto gains.\n",
        "Negative Growth Insights: While the relationship between 'High' and 'Close' is expected to be positive, a significant spread of points below the regression line could indicate months where, despite reaching a high price, the stock experienced a notable sell-off by the end of the month, leading to a significantly lower closing price. This \"failure\" to close near the high could be a sign of bearish pressure or negative sentiment, which, if frequent, points to periods or overall patterns of negative growth or difficulty in sustaining positive price movements."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Chart - 6 visualization code\n",
        "\n",
        "# üìä Visualize the relationship between Low and Close prices using a Scatter Plot with Regression Line\n",
        "plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "sns.regplot(data=df, x='Low', y='Close', scatter_kws={'alpha':0.6}, line_kws={\"color\": \"green\"}) # Create the scatter plot with a regression line\n",
        "plt.title('üìâ Relationship between Monthly Low and Close Prices', fontsize=16) # Add a title\n",
        "plt.xlabel('Low Price', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regplot is used to show the linear relationship and trend between 'Low' and 'Close' prices."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals the correlation between the lowest price hit in a month and the closing price. Indicates how often the closing price is near the monthly low or rebounds significantly from it."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: A strong positive correlation means months with higher lows tend to close higher, suggesting support levels. Negative: Points far above the line (Close much higher than Low) could indicate strong rebounds, but also potential volatility. If lows are consistently decreasing while closing prices are also low, it indicates a strong negative trend."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Chart - 7 visualization code\n",
        "\n",
        "# üìä Visualize High-Low Difference and Open-Close Difference over Time\n",
        "plt.figure(figsize=(14, 7)) # Set the figure size\n",
        "\n",
        "# Subplot 1: High-Low Difference over Time\n",
        "plt.subplot(2, 1, 1) # 2 rows, 1 column, 1st plot\n",
        "sns.lineplot(data=df, x='Date', y='High_Low_Diff') # Line plot for High-Low Diff\n",
        "plt.title('üìà Monthly High-Low Difference Over Time', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('High-Low Difference', fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "# Subplot 2: Open-Close Difference over Time\n",
        "plt.subplot(2, 1, 2) # 2 rows, 1 column, 2nd plot\n",
        "sns.lineplot(data=df, x='Date', y='Open_Close_Diff', color='orange') # Line plot for Open-Close Diff\n",
        "plt.title('üìä Monthly Open-Close Difference Over Time', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Open-Close Difference', fontsize=12)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout() # Adjust layout\n",
        "plt.show() # Display the plots"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line plots are used to visualize the trend of numerical variables (High_Low_Diff and Open_Close_Diff) over a continuous time axis (Date)."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe trends in volatility (High-Low Difference) ‚Äì are months getting more or less volatile over time? See if there are periods with consistently positive or negative Open-Close Differences, indicating sustained upward or downward monthly price movements.\n",
        "\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying periods of lower volatility can inform less risky trading strategies. Recognizing periods with predominantly negative Open-Close Difference suggests a general upward trend within months, which is positive. Negative: Spikes in High-Low Difference indicate increased volatility and risk. Sustained periods with predominantly positive Open-Close Difference mean months often end lower than they began, reflecting a negative trend within those months."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# üìä Visualize the relationship between High-Low Difference and Close Price\n",
        "plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "sns.scatterplot(data=df, x='High_Low_Diff', y='Close', alpha=0.6) # Create the scatter plot\n",
        "plt.title('üìà Relationship between Monthly Volatility (High-Low Diff) and Closing Price', fontsize=16) # Add a title\n",
        "plt.xlabel('High-Low Difference', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is used to visualize the relationship between High_Low_Diff (a measure of monthly volatility) and the target variable Close. It helps determine if there's a pattern or correlation between how much the stock price moves within a month and the final closing price."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can observe if months with higher volatility tend to be associated with higher or lower closing prices, or if there's no clear relationship. The spread of points shows the variability in closing prices for similar levels of volatility."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: If higher volatility is often associated with higher closing prices, it could suggest that upward price movements are often accompanied by large intraday/intra-month swings, which could inform trading strategies focused on volatile periods. Negative: If high volatility is associated with a wide range of closing prices, it indicates that high volatility doesn't guarantee a favorable closing price and could imply increased risk. If high volatility frequently leads to lower closing prices (points clustered in the bottom-right), it's a strong indicator of negative growth potential during volatile periods."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Chart - 9 visualization code\n",
        "\n",
        "# üìä Visualize the relationship between Open-Close Difference and Close Price\n",
        "plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "sns.scatterplot(data=df, x='Open_Close_Diff', y='Close', alpha=0.6) # Create the scatter plot\n",
        "plt.title('üìä Relationship between Monthly Price Movement (Open-Close Diff) and Closing Price', fontsize=16) # Add a title\n",
        "plt.xlabel('Open-Close Difference', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is used to visualize the relationship between Open_Close_Diff (indicating monthly price gain/loss) and the target variable Close. It helps understand how the magnitude and direction of the monthly price change relates to the final closing value."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can observe if months with large positive Open_Close_Diff (meaning the price dropped significantly within the month) tend to have lower closing prices, and if months with large negative Open_Close_Diff (meaning the price increased significantly within the month) tend to have higher closing prices. The concentration of points around zero Open_Close_Diff would indicate months where the open and close prices were similar."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: If a clear pattern emerges where negative Open_Close_Diff values (monthly gains) strongly correlate with higher closing prices, it reinforces the idea that periods of upward monthly movement result in favorable closing prices. This can inform bullish strategies. Negative: If months with significant positive Open_Close_Diff (monthly losses) cluster heavily at low closing prices, it directly highlights periods of negative growth and the impact of downward monthly price swings. A wide scatter of points for similar Open_Close_Diff values would suggest that the monthly movement isn't the sole determinant of the closing price and other factors play a role, implying less predictability."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Chart - 10 visualization code\n",
        "\n",
        "# üìä Visualize the distribution of Closing Price by Month using Violin Plots\n",
        "plt.figure(figsize=(14, 7)) # Set the figure size\n",
        "sns.violinplot(data=df, x='Month_Name', y='Close', palette='coolwarm', order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']) # Create the violin plot\n",
        "plt.title('üéª Distribution of Monthly Closing Price by Month', fontsize=16) # Add a title\n",
        "plt.xlabel('Month', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.grid(axis='y', alpha=0.75) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Violin plots show the distribution of a numerical variable across different categories (in this case, Close price by Month_Name). They are more visually appealing than box plots for showing the density of the data at different price points within each month, highlighting multimodality if present."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe how the distribution of closing prices varies across different months of the year. Identify months that tend to have higher or lower median closing prices, or months with wider or narrower distributions (indicating volatility). See if there are any months with unusual clusters of closing prices or potential outliers."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying months where the stock price distribution is generally higher or less volatile can inform favorable trading or investment timing. Understanding seasonal patterns might lead to strategies capitalizing on typical monthly movements. Negative: Months where the distribution is centered at lower prices, or has a wider spread towards lower values, indicate periods of potential negative growth or higher risk. If certain months consistently show lower price distributions over the years, it suggests a seasonal negative trend."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# üìä Visualize Open, High, Low, and Close Prices over Time\n",
        "plt.figure(figsize=(14, 7)) # Set the figure size\n",
        "\n",
        "sns.lineplot(data=df, x='Date', y='Open', label='Open') # Line plot for Open Price\n",
        "sns.lineplot(data=df, x='Date', y='High', label='High') # Line plot for High Price\n",
        "sns.lineplot(data=df, x='Date', y='Low', label='Low')   # Line plot for Low Price\n",
        "sns.lineplot(data=df, x='Date', y='Close', label='Close') # Line plot for Close Price\n",
        "\n",
        "plt.title('üìà Open, High, Low, and Closing Prices Over Time', fontsize=16) # Add a title\n",
        "plt.xlabel('Date', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Price', fontsize=12) # Label the y-axis\n",
        "plt.legend() # Add a legend to distinguish the lines\n",
        "plt.grid(True) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multi-line plot is used to compare the trends of multiple numerical variables (Open, High, Low, Close) over a continuous time period (Date). This allows for easy visual comparison of their movements relative to each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the relationship and relative positions of the Open, High, Low, and Close lines over time. See how the spread between High and Low (volatility) changes. Note if the Close price tends to follow the Open price closely or deviate significantly. Identify periods where all prices move together in a strong trend."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Seeing consistent upward trends across all four price points indicates strong positive growth periods, informing buy decisions. Observing that Close price often stays near the High price could indicate bullish sentiment and strength. Negative: Periods where the lines consistently trend downwards represent negative growth. If the Close price frequently ends near the Low price, it suggests bearish pressure and difficulty in maintaining value, indicating negative sentiment."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# üìä Visualize Average Closing Price per Year using a Bar Plot\n",
        "plt.figure(figsize=(12, 6)) # Set the figure size\n",
        "\n",
        "# Calculate the average closing price per year\n",
        "avg_close_by_year = df.groupby('Year')['Close'].mean().reset_index()\n",
        "\n",
        "sns.barplot(data=avg_close_by_year, x='Year', y='Close', palette='viridis') # Create the bar plot\n",
        "plt.title('üìä Average Yes Bank Stock Closing Price per Year', fontsize=16) # Add a title\n",
        "plt.xlabel('Year', fontsize=12) # Label the x-axis\n",
        "plt.ylabel('Average Closing Price', fontsize=12) # Label the y-axis\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels\n",
        "plt.grid(axis='y', alpha=0.75) # Add a grid\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is effective for comparing a numerical value (average closing price) across distinct categories (each year). It provides a clear visual comparison of the average stock value in each year."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly see the average closing price for each year and how it changes from one year to the next. Easily identify years with the highest and lowest average closing prices. Observe periods of sustained increase or decrease in the average yearly price."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Years with significantly taller bars than previous years indicate strong positive growth in the average stock value for that year, informing long-term investment perspective. Negative: Years with significantly shorter bars than previous years clearly highlight periods of negative growth in the average stock value. A downward trend in the bar heights over consecutive years is a strong indicator of sustained negative performance."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# üìä Visualize Average High-Low Difference and Open-Close Difference per Year\n",
        "plt.figure(figsize=(14, 7)) # Set the figure size\n",
        "\n",
        "# Calculate the average engineered features per year\n",
        "avg_engineered_by_year = df.groupby('Year')[['High_Low_Diff', 'Open_Close_Diff']].mean().reset_index()\n",
        "\n",
        "# Subplot 1: Average High-Low Difference per Year\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n",
        "sns.barplot(data=avg_engineered_by_year, x='Year', y='High_Low_Diff', palette='coolwarm')\n",
        "plt.title('üìà Average Monthly High-Low Difference per Year', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Average High-Low Difference', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Subplot 2: Average Open-Close Difference per Year\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n",
        "sns.barplot(data=avg_engineered_by_year, x='Year', y='Open_Close_Diff', palette='viridis')\n",
        "plt.title('üìä Average Monthly Open-Close Difference per Year', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Average Open-Close Difference', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "plt.tight_layout() # Adjust layout\n",
        "plt.show() # Display the plots"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are used to compare the average values of High_Low_Diff and Open_Close_Diff across different years, providing a clear visual comparison of how these metrics have trended annually."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe which years had the highest or lowest average monthly volatility (High_Low_Diff). See if there are years where the average monthly price movement (Open_Close_Diff) was significantly positive (indicating average monthly loss) or negative (indicating average monthly gain). Identify periods where the stock was consistently more or less volatile or showed a consistent average monthly trend."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying years with lower average volatility can indicate periods of lower risk. Years with a consistent negative average Open_Close_Diff suggest that months in that year typically ended higher than they opened, which is a positive sign for investors. Negative: Years with high average High_Low_Diff indicate increased risk due to volatility. Years with a consistent positive average Open_Close_Diff mean months in that year typically ended lower than they opened, indicating a period of average monthly losses and negative growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 14 - Correlation Heatmap visualization code\n",
        "\n",
        "# üìä Compute the correlation matrix\n",
        "# Select only the numerical columns for the heatmap\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8)) # Set the figure size\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('üìä Correlation Heatmap of Numerical Features', fontsize=16) # Add a title\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is used to visualize the pairwise correlation coefficients between multiple numerical variables. It provides a quick overview of how strongly and in what direction each pair of variables is linearly related. This is essential for identifying multicollinearity among features and understanding relationships with the target variable (Close)."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "identify variables that are highly correlated with the target variable (Close). Look for strong positive or negative correlations between predictor variables (e.g., between Open, High, Low). Understand the strength of the relationship between the engineered features (High_Low_Diff, Open_Close_Diff) and other variables, including the target."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 - Pair Plot visualization code\n",
        "\n",
        "# üìä Create a Pair Plot of the numerical features\n",
        "# Select only the numerical columns for the pair plot\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "# It's often good to exclude the 'Year' and 'Month' for pair plot if they are just identifiers,\n",
        "# but in this case, they might show trends related to price. Let's include them.\n",
        "# You might exclude High_Low_Diff and Open_Close_Diff if the plot becomes too crowded,\n",
        "# but they are engineered features, so let's include them to see relationships.\n",
        "\n",
        "# Consider limiting the number of columns if the dataset is large to keep the plot readable.\n",
        "# For this dataset, including all numerical columns should be fine.\n",
        "\n",
        "sns.pairplot(df[numerical_cols], diag_kind='kde') # Create the pair plot\n",
        "plt.suptitle('üìä Pair Plot of Numerical Features', y=1.02, fontsize=16) # Add a title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a matrix of scatter plots for each pair of variables and histograms/KDEs for the diagonal elements. It provides a comprehensive visual overview of the relationships between all numerical variables and their individual distributions in a single visualization."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quickly see the relationships between all pairs of numerical features, including how each predictor relates to the target variable (Close). Observe the distribution of each individual numerical feature (on the diagonal). Identify potential linear or non-linear relationships and clusters in the data. It complements the correlation heatmap by showing the actual scatter of points."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average closing price of Yes Bank stock is significantly different in years after 2018 (when the fraud case became prominent) compared to years before 2018."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 1\n",
        "\n",
        "from scipy.stats import ttest_ind # Import the independent t-test function\n",
        "\n",
        "# Define the two groups: Closing prices before and after 2018\n",
        "# Ensure df['Year'] is treated as a number for comparison\n",
        "df['Year'] = pd.to_numeric(df['Year'])\n",
        "\n",
        "close_before_2018 = df[df['Year'] < 2018]['Close']\n",
        "close_after_2018 = df[df['Year'] >= 2018]['Close']\n",
        "\n",
        "# Perform the independent samples t-test\n",
        "# We set equal_var=False to perform Welch's t-test, which does not assume equal variances.\n",
        "# This is generally safer unless you have strong evidence of equal variances.\n",
        "t_statistic, p_value = ttest_ind(close_before_2018, close_after_2018, equal_var=False)\n",
        "\n",
        "print(f\"Independent Samples t-test Results for Hypothesis 1:\")\n",
        "print(f\"  Mean Closing Price Before 2018: {close_before_2018.mean():.2f}\")\n",
        "print(f\"  Mean Closing Price After 2018: {close_after_2018.mean():.2f}\")\n",
        "print(f\"  T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"  P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05 # Set a significance level (commonly 0.05)\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "if p_value < alpha:\n",
        "    print(f\"  The p-value ({p_value:.4f}) is less than the significance level ({alpha}).\")\n",
        "    print(\"  We reject the null hypothesis.\")\n",
        "    print(\"  Conclusion: The average closing price of Yes Bank stock is significantly different in years after 2018 compared to years before 2018.\")\n",
        "else:\n",
        "    print(f\"  The p-value ({p_value:.4f}) is greater than or equal to the significance level ({alpha}).\")\n",
        "    print(\"  We fail to reject the null hypothesis.\")\n",
        "    print(\"  Conclusion: There is not enough statistical evidence to conclude that the average closing price of Yes Bank stock is significantly different in years after 2018 compared to years before 2018.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Independent Samples t-test (specifically, Welch's t-test)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Independent Samples t-test because:\n",
        "\n",
        "It compares means: Our hypothesis is about the average closing price. The t-test is designed to compare the means of two groups.\n",
        "Independent groups: The data points in the \"before 2018\" group are independent of the data points in the \"after 2018\" group; the closing price in one period doesn't directly determine the closing price in the other in a paired way.\n",
        "Continuous data: The variable being compared (Close) is continuous (a numerical price)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis ($H_0$$H_0$): There is no statistically significant linear relationship between the High price and the Close price of Yes Bank stock. (The true population correlation coefficient is zero.)\n",
        "\n",
        "Alternate Hypothesis ($H_a$$H_a$): There is a statistically significant linear relationship between the High price and the Close price of Yes Bank stock. (The true population correlation coefficient is not zero.)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 2\n",
        "\n",
        "from scipy.stats import pearsonr # Import the Pearson correlation function\n",
        "\n",
        "# Perform the Pearson correlation test between 'High' and 'Close' prices\n",
        "correlation_coefficient, p_value = pearsonr(df['High'], df['Close'])\n",
        "\n",
        "print(f\"Pearson Correlation Test Results for Hypothesis 2:\")\n",
        "print(f\"  Correlation Coefficient (High vs. Close): {correlation_coefficient:.4f}\")\n",
        "print(f\"  P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05 # Set a significance level (commonly 0.05)\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "if p_value < alpha:\n",
        "    print(f\"  The p-value ({p_value:.4f}) is less than the significance level ({alpha}).\")\n",
        "    print(\"  We reject the null hypothesis.\")\n",
        "    print(\"  Conclusion: There is a statistically significant linear relationship between the High price and the Close price of Yes Bank stock.\")\n",
        "else:\n",
        "    print(f\"  The p-value ({p_value:.4f}) is greater than or equal to the significance level ({alpha}).\")\n",
        "    print(\"  We fail to reject the null hypothesis.\")\n",
        "    print(\"  Conclusion: There is not enough statistical evidence to conclude that there is a statistically significant linear relationship between the High price and the Close price of Yes Bank stock.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothetical Statement 2, the statistical test used is the Pearson Correlation Test.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Linear Relationship: Our hypothesis specifically concerns a linear relationship between High and Close. The Pearson correlation coefficient is the standard measure for the strength and direction of a linear association between two continuous variables.\n",
        "Two Continuous Variables: The test is appropriate for analyzing the relationship between two continuous numerical variables, which High and Close both are.\n",
        "Testing for Significance: The test provides a p-value to determine if the observed linear correlation in our sample data is strong enough to conclude that a linear relationship exists in the overall population (i.e., the correlation coefficient is statistically different from zero)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis ($H_0$$H_0$): The average monthly volatility (High_Low_Diff) is not significantly different between months where the stock closed lower than it opened and months where it closed higher than it opened. (The true population mean High_Low_Diff is the same for both groups.)\n",
        "\n",
        "Alternate Hypothesis ($H_a$$H_a$): The average monthly volatility (High_Low_Diff) is significantly different between months where the stock closed lower than it opened and months where it closed higher than it opened. (The true population mean High_Low_Diff is different for the two groups.)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 3\n",
        "\n",
        "from scipy.stats import ttest_ind # Import the independent t-test function\n",
        "\n",
        "# Define the two groups based on Open-Close Difference:\n",
        "# Group 1: Months where Close < Open (Open_Close_Diff > 0)\n",
        "volatility_when_close_lower = df[df['Open_Close_Diff'] > 0]['High_Low_Diff']\n",
        "\n",
        "# Group 2: Months where Close > Open (Open_Close_Diff < 0)\n",
        "volatility_when_close_higher = df[df['Open_Close_Diff'] < 0]['High_Low_Diff']\n",
        "\n",
        "# Note: Months where Open == Close (Open_Close_Diff == 0) are excluded from both groups.\n",
        "# Check if either group is empty before performing the test\n",
        "if volatility_when_close_lower.empty or volatility_when_close_higher.empty:\n",
        "    print(\"Cannot perform t-test: One or both groups are empty.\")\n",
        "    print(f\"  Months with Close < Open: {len(volatility_when_close_lower)}\")\n",
        "    print(f\"  Months with Close > Open: {len(volatility_when_close_higher)}\")\n",
        "else:\n",
        "    # Perform the independent samples t-test (using Welch's t-test)\n",
        "    t_statistic, p_value = ttest_ind(volatility_when_close_lower, volatility_when_close_higher, equal_var=False)\n",
        "\n",
        "    print(f\"Independent Samples t-test Results for Hypothesis 3:\")\n",
        "    print(f\"  Mean High-Low Diff when Close < Open: {volatility_when_close_lower.mean():.2f}\")\n",
        "    print(f\"  Mean High-Low Diff when Close > Open: {volatility_when_close_higher.mean():.2f}\")\n",
        "    print(f\"  T-statistic: {t_statistic:.4f}\")\n",
        "    print(f\"  P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpret the results\n",
        "    alpha = 0.05 # Set a significance level (commonly 0.05)\n",
        "\n",
        "    print(\"\\nConclusion:\")\n",
        "    if p_value < alpha:\n",
        "        print(f\"  The p-value ({p_value:.4f}) is less than the significance level ({alpha}).\")\n",
        "        print(\"  We reject the null hypothesis.\")\n",
        "        print(\"  Conclusion: The average monthly volatility (High_Low_Diff) is significantly different between months where the stock closed lower than it opened and months where it closed higher than it opened.\")\n",
        "    else:\n",
        "        print(f\"  The p-value ({p_value:.4f}) is greater than or equal to the significance level ({alpha}).\")\n",
        "        print(\"  We fail to reject the null hypothesis.\")\n",
        "        print(\"  Conclusion: There is not enough statistical evidence to conclude that the average monthly volatility (High_Low_Diff) is significantly different between months where the stock closed lower than it opened and months where it closed higher than it opened.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Samples t-test (specifically, Welch's t-test).\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen because we are comparing the means of a continuous variable (High_Low_Diff) between two independent groups of months, defined by the sign of Open_Close_Diff. The t-test is designed for this mean comparison, and Welch's version is used for robustness against unequal variances"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üßΩ 7. Handle missing values if any (Check again after feature engineering)\n",
        "print(\"\\nMissing values before ffill:\")\n",
        "print(df.isnull().sum())\n",
        "# For numerical columns, fill with forward fill (or you can use mean/median)\n",
        "# You might want to be careful with ffill on stock data if there are long gaps\n",
        "# Given this is monthly data, ffill might be acceptable, but consider alternatives\n",
        "# like imputation with mean/median if you have significant missing data.\n",
        "df.fillna(method='ffill', inplace=True)\n",
        "print(\"\\nMissing values after ffill:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the code you have in your notebook, you have used the Forward Fill (ffill) method for missing value imputation.\n",
        "\n",
        "Why Forward Fill (ffill)?\n",
        "\n",
        "You used df.fillna(method='ffill', inplace=True). This technique is commonly used in time series data because it propagates the last valid observation forward.\n",
        "\n",
        "Reasoning: In the context of stock prices, using the previous month's value (ffill) to fill a missing value can be a reasonable assumption. It reflects the idea that prices tend to maintain a certain level unless there's a new event, and using a past value preserves the temporal sequence of the data. This is often more suitable for time series than using a simple mean or median of the entire dataset, which would not account for trends or seasonality. Your comment in the code \"Given this is monthly data, ffill might be acceptable\" aligns with this reasoning."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments Code\n",
        "\n",
        "# Let's identify potential outliers using the Interquartile Range (IQR) method\n",
        "# This method is good for skewed data and doesn't assume a normal distribution.\n",
        "\n",
        "# Columns where you want to check for outliers (e.g., Open, High, Low, Close, engineered features)\n",
        "outlier_check_cols = ['Open', 'High', 'Low', 'Close', 'High_Low_Diff', 'Open_Close_Diff']\n",
        "\n",
        "print(\"--- Checking for Outliers (IQR Method) ---\")\n",
        "\n",
        "for col in outlier_check_cols:\n",
        "    print(f\"\\nChecking column: {col}\")\n",
        "\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "    print(f\"  Q1 ({col}): {Q1:.2f}\")\n",
        "    print(f\"  Q3 ({col}): {Q3:.2f}\")\n",
        "    print(f\"  IQR ({col}): {IQR:.2f}\")\n",
        "    print(f\"  Lower Bound: {lower_bound:.2f}\")\n",
        "    print(f\"  Upper Bound: {upper_bound:.2f}\")\n",
        "    print(f\"  Number of outliers detected: {len(outliers)}\")\n",
        "\n",
        "    # Optional: Display the outlier rows for inspection\n",
        "    # if not outliers.empty:\n",
        "    #     print(\"  Sample Outliers:\")\n",
        "    #     print(outliers[[col, 'Date']].head()) # Display the outlier value and date\n",
        "\n",
        "# --- Outlier Treatment Techniques (Choose ONE method per column, or decide not to treat) ---\n",
        "# Note: Applying treatments will modify your DataFrame 'df'.\n",
        "# Decide based on your analysis of the detected outliers.\n",
        "\n",
        "# Example Treatment Option 1: Capping (Winsorization) - replace outliers with the bound values\n",
        "# This is often preferred for stock data as it keeps the data points but reduces their extreme influence.\n",
        "# for col in outlier_check_cols:\n",
        "#     Q1 = df[col].quantile(0.25)\n",
        "#     Q3 = df[col].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "#     df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
        "#     df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
        "# print(\"\\n--- Applied Outlier Capping (IQR Method) ---\")\n",
        "\n",
        "\n",
        "# Example Treatment Option 2: Removal - remove the rows containing outliers\n",
        "# Use this cautiously as it can reduce your dataset size, especially if outliers are frequent.\n",
        "# initial_rows = df.shape[0]\n",
        "# for col in outlier_check_cols:\n",
        "#     Q1 = df[col].quantile(0.25)\n",
        "#     Q3 = df[col].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "#     df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "# # Reset index after removing rows\n",
        "# df = df.reset_index(drop=True)\n",
        "# print(f\"\\n--- Applied Outlier Removal (IQR Method) ---\")\n",
        "# print(f\"  Rows removed: {initial_rows - df.shape[0]}\")\n",
        "\n",
        "\n",
        "# Example Treatment Option 3: Transformation (e.g., Log Transformation)\n",
        "# This can make the data distribution more symmetric and reduce the impact of extreme values, but it changes the scale.\n",
        "# for col in ['Open', 'High', 'Low', 'Close']: # Apply to original price columns\n",
        "#      df[col + '_log'] = np.log1p(df[col]) # Using log1p handles zero values\n",
        "# print(\"\\n--- Applied Log Transformation (as a form of outlier treatment) ---\")\n",
        "\n",
        "\n",
        "# Check the shape and descriptive statistics after treatment if you applied one\n",
        "# print(\"\\nShape after outlier treatment:\", df.shape)\n",
        "# print(\"\\nDescriptive statistics after outlier treatment:\")\n",
        "# print(df.describe())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capping (Winsorization): Replacing outliers with the upper or lower bounds determined by the IQR method.\n",
        "Removal: Deleting the rows where outliers are detected.\n",
        "Transformation: Applying mathematical transformations like log transformation."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# --- Check for categorical columns first ---\n",
        "print(\"Data types before encoding:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nUnique values in 'Month_Name' before encoding:\", df['Month_Name'].unique())\n",
        "\n",
        "\n",
        "# Perform One-Hot Encoding for the 'Month_Name' column\n",
        "# drop_first=True is often used to avoid multicollinearity\n",
        "df = pd.get_dummies(df, columns=['Month_Name'], drop_first=True)\n",
        "\n",
        "print(\"\\nData types after encoding:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nShape after encoding:\", df.shape)\n",
        "print(\"\\nFirst 5 rows after encoding:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Techniques Used: You have used One-Hot Encoding using pd.get_dummies().\n",
        "Why used:\n",
        "Handles Nominal Data: One-hot encoding is suitable for nominal categorical variables (categories without a specific order), or when you don't want to impose an arbitrary ordinal relationship. While months have a cyclical order, one-hot encoding treats each month as a distinct category, which can be effective for capturing potential monthly patterns without assuming a linear trend.\n",
        "Avoids Misinterpretation: It prevents models from misinterpreting ordinal relationships (e.g., thinking that 'December' is numerically greater than 'January' in a way that impacts the outcome linearly).\n",
        "Compatibility: Many machine learning algorithms work well with one-hot encoded features.\n",
        "Using drop_first=True helps to avoid multicollinearity, which can be an issue for some linear models, though tree-based models are less sensitive to it."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# --- Review of Engineered Features ---\n",
        "print(\"Engineered Features created during Data Wrangling:\")\n",
        "print(\" - High_Low_Diff: Represents the range of price movement within a month (High - Low).\")\n",
        "print(\" - Open_Close_Diff: Represents the difference between the opening and closing price (Open - Close), indicating monthly gain/loss.\")\n",
        "\n",
        "# You have already created these features in the Data Wrangling section.\n",
        "# If you were to create more features here, you would add code like:\n",
        "# df['Avg_Daily_Price'] = (df['High'] + df['Low']) / 2\n",
        "# df['Price_Change_Percentage'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "\n",
        "# --- Minimizing Feature Correlation ---\n",
        "print(\"\\nMinimizing feature correlation (multicollinearity) is important for some models (like Linear Regression).\")\n",
        "print(\"Highly correlated features can make model coefficients unstable.\")\n",
        "print(\"Techniques to address multicollinearity will be considered during Feature Selection and/or Dimensionality Reduction.\")\n",
        "print(\"The correlation heatmap (Chart 14) is a key tool for identifying highly correlated features.\")\n",
        "\n",
        "\n",
        "# Display the first few rows with the engineered features\n",
        "print(\"\\nFirst 5 rows showing engineered features:\")\n",
        "print(df[['Open', 'High', 'Low', 'Close', 'High_Low_Diff', 'Open_Close_Diff']].head())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# --- Define Target Variable and Potential Predictor Features ---\n",
        "\n",
        "# The target variable is 'Close'\n",
        "target = 'Close'\n",
        "\n",
        "# Potential predictor features - exclude the target and original 'Date' column,\n",
        "# and the original 'Month_Name' column (as it's been encoded).\n",
        "# We'll include all other numerical and one-hot encoded columns initially.\n",
        "predictor_features = [col for col in df.columns if col not in [target, 'Date']]\n",
        "\n",
        "print(\"Potential Predictor Features:\")\n",
        "print(predictor_features)\n",
        "\n",
        "# --- Feature Selection Strategy (Based on EDA and Correlation) ---\n",
        "# Based on your EDA (scatter plots, correlation heatmap), you can make decisions here.\n",
        "\n",
        "# Example Strategy:\n",
        "# 1. Include features highly correlated with the target ('Close').\n",
        "# 2. Consider excluding one of a pair of highly correlated predictor features to reduce multicollinearity (if using models sensitive to it, like Linear Regression).\n",
        "# 3. Include engineered features if they show good relationship with the target or add valuable information.\n",
        "# 4. Decide whether to include time-based features ('Year', encoded 'Month_Name') based on observed trends and potential seasonality.\n",
        "\n",
        "# Example Code based on likely insights from the correlation heatmap:\n",
        "# (You'll need to adjust this based on your actual heatmap results)\n",
        "\n",
        "# Assuming High, Low, and Open are highly correlated with Close (which is typical)\n",
        "# Assuming Open, High, Low are highly correlated with each other.\n",
        "# You might choose to keep Open, High, and Low as they are direct price inputs.\n",
        "# You might choose to keep only one (e.g., Open) to represent the starting price.\n",
        "\n",
        "# Let's create a selected features list based on common approaches for stock data:\n",
        "selected_features = [\n",
        "    'Open',          # Starting price\n",
        "    'High',          # Highest price (highly correlated with Close)\n",
        "    'Low',           # Lowest price (highly correlated with Close)\n",
        "    'High_Low_Diff', # Volatility\n",
        "    'Open_Close_Diff' # Monthly movement\n",
        "    # Include encoded months if seasonal patterns were strong in Chart 10\n",
        "    # 'Month_Name_February', 'Month_Name_March', ...\n",
        "    # Include Year if there's a strong long-term trend not captured by price\n",
        "    # 'Year'\n",
        "]\n",
        "\n",
        "# Filter out any selected features that don't exist in the DataFrame after encoding\n",
        "selected_features = [feature for feature in selected_features if feature in df.columns]\n",
        "\n",
        "\n",
        "print(\"\\nSelected Predictor Features:\")\n",
        "print(selected_features)\n",
        "\n",
        "# --- Create Feature Matrix (X) and Target Vector (y) ---\n",
        "X = df[selected_features]\n",
        "y = df[target]\n",
        "\n",
        "print(\"\\nShape of Feature Matrix (X):\", X.shape)\n",
        "print(\"Shape of Target Vector (y):\", y.shape)\n",
        "\n",
        "# Display first few rows of X and y\n",
        "print(\"\\nFirst 5 rows of X:\")\n",
        "print(X.head())\n",
        "print(\"\\nFirst 5 rows of y:\")\n",
        "print(y.head())"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the code I provided for Feature Selection, the primary method used is Manual Feature Selection based on EDA and Correlation Analysis.\n",
        "\n",
        "Method: Manual selection by creating a list selected_features based on insights gained from the Exploratory Data Analysis (EDA) and the Correlation Heatmap (Chart 14).\n",
        "Why used:\n",
        "Insights from EDA/Correlation: You've visually inspected the data, trends, distributions, and correlations. The correlation heatmap specifically provides quantitative measures of linear relationships between features and the target, guiding which features are likely good predictors.\n",
        "Domain Knowledge: For stock data, features like Open, High, and Low are inherently important for predicting Close. Engineered features like High-Low Difference and Open-Close Difference capture relevant aspects like volatility and monthly momentum.\n",
        "Simplicity: For a dataset with a relatively small number of features like this one, manual selection based on understanding the data is a practical and often effective starting point.\n",
        "Avoiding Multicollinearity: While not a strict algorithm within this manual process, the insights from the correlation heatmap help you consider multicollinearity and potentially avoid including features that are very redundant, especially if using models sensitive to this."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on a typical analysis of stock price data and the insights you would likely gain from the EDA and Correlation Heatmap in your notebook:\n",
        "\n",
        "Important Features and Why:\n",
        "\n",
        "Open, High, Low:\n",
        "\n",
        "Why Important: These are almost always the most important predictors for the Close price in stock data. The closing price of a period is highly dependent on the price movements that occurred during that period, captured by the opening price, the highest point reached, and the lowest point reached. Your correlation heatmap (Chart 14) will almost certainly show very high positive correlations between Close and Open, High, and Low.\n",
        "High_Low_Diff:\n",
        "\n",
        "Why Important: This engineered feature captures the monthly volatility or trading range. Higher volatility might be associated with different closing price behaviors than low volatility. While its direct correlation with Close might not be as high as Open, High, or Low, it provides additional information about the price action within the month, which can be valuable for the model.\n",
        "Open_Close_Diff:\n",
        "\n",
        "Why Important: This engineered feature indicates the net price movement within the month (whether the stock went up or down from open to close and by how much). This directly relates to the change you are trying to predict (the Close price relative to the Open price). Its relationship with Close might be complex, but it provides a summary of the month's overall performance.\n",
        "Year (Potentially):\n",
        "\n",
        "Why Important (Conditional): If your time series plots (like Chart 1 and Chart 12 showing price over time and average price by year) show a strong long-term trend (upward or downward) across the years, the Year feature can help the model capture this trend.\n",
        "Encoded Month_Name features (Potentially):\n",
        "\n",
        "Why Important (Conditional): If your violin plots (like Chart 10 showing distribution by month) or other seasonal analyses reveal distinct patterns or average price differences tied to specific months (e.g., the stock tends to perform better in December), the one-hot encoded month features can help the model capture this seasonality.\n",
        "In summary, based on the typical structure of stock data:\n",
        "\n",
        "Open, High, and Low are foundational features due to their direct relationship with the Close price.\n",
        "High_Low_Diff and Open_Close_Diff are important as they summarize key aspects of monthly price dynamics (volatility and net change).\n",
        "Year and Month_Name are important if there are clear long-term trends or seasonal patterns in the data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# --- Check distributions first to see if transformation is needed ---\n",
        "# Use histograms or displots to visualize distributions of numerical columns.\n",
        "# sns.histplot(df['Close'], kde=True)\n",
        "# plt.title('Distribution of Close Price')\n",
        "# plt.show()\n",
        "# sns.histplot(df['High_Low_Diff'], kde=True) # You already did this in Chart 3\n",
        "\n",
        "# --- Apply Transformation (Example using Log Transformation) ---\n",
        "# Select columns to transform (exclude Year and Month if they are just identifiers, keep prices and engineered features)\n",
        "cols_to_transform = ['Open', 'High', 'Low', 'Close', 'High_Low_Diff', 'Open_Close_Diff']\n",
        "\n",
        "print(\"Applying Log Transformation (log1p) to selected numerical columns...\")\n",
        "\n",
        "for col in cols_to_transform:\n",
        "    # Check if column exists and has non-negative values (log1p requires x >= -1)\n",
        "    if col in df.columns and (df[col] >= -1).all():\n",
        "         df[col + '_log'] = np.log1p(df[col])\n",
        "         print(f\"  Transformed '{col}' to '{col}_log'\")\n",
        "    else:\n",
        "         print(f\"  Skipped transformation for '{col}' (not found or has values < -1)\")\n",
        "\n",
        "\n",
        "# You would likely keep the original columns AND the transformed columns initially,\n",
        "# then decide during feature selection which set to use for modeling.\n",
        "# Or you could replace the original columns with the transformed ones if preferred.\n",
        "\n",
        "# Example: Replace original with transformed (be careful, this overwrites)\n",
        "# for col in cols_to_transform:\n",
        "#      if col + '_log' in df.columns:\n",
        "#          df[col] = df[col + '_log']\n",
        "#          df.drop(columns=[col + '_log'], inplace=True)\n",
        "\n",
        "\n",
        "print(\"\\nShape after transformation:\", df.shape)\n",
        "print(\"\\nFirst 5 rows after transformation (showing new _log columns if applied):\")\n",
        "print(df.head())\n",
        "\n",
        "# --- Re-check distributions after transformation ---\n",
        "# Use histograms/displots again to see the effect of the transformation.\n",
        "# sns.histplot(df['Close_log'], kde=True)\n",
        "# plt.title('Distribution of Transformed Close Price')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler # Already imported, but good to show it's used here\n",
        "\n",
        "# --- Identify Numerical Features to Scale ---\n",
        "# Exclude the target variable ('Close' or 'Close_log' if you transformed it and replaced)\n",
        "# Exclude the 'Date' column\n",
        "# Exclude the one-hot encoded month columns (they are binary 0 or 1, don't need scaling)\n",
        "\n",
        "# Determine which columns are numerical and *should* be scaled.\n",
        "# This will depend on whether you replaced original columns with transformed ones.\n",
        "\n",
        "# If you DID NOT replace original columns with _log columns:\n",
        "numerical_cols_to_scale = ['Open', 'High', 'Low', 'High_Low_Diff', 'Open_Close_Diff', 'Year']\n",
        "# Add _log columns if you kept them alongside originals and want to scale them too\n",
        "# numerical_cols_to_scale.extend(['Open_log', 'High_log', 'Low_log', 'Close_log', 'High_Low_Diff_log', 'Open_Close_Diff_log'])\n",
        "# Remove the target column from this list ('Close' or 'Close_log') depending on which you're using as target\n",
        "if 'Close' in numerical_cols_to_scale:\n",
        "    numerical_cols_to_scale.remove('Close')\n",
        "if 'Close_log' in numerical_cols_to_scale:\n",
        "    numerical_cols_to_scale.remove('Close_log')\n",
        "\n",
        "\n",
        "# If you DID replace original columns with _log columns:\n",
        "# numerical_cols_to_scale = ['Open', 'High', 'Low', 'High_Low_Diff', 'Open_Close_Diff', 'Year'] # These are now the _log transformed ones\n",
        "# Remove the target column from this list ('Close' is now the transformed one)\n",
        "# if 'Close' in numerical_cols_to_scale: # This would be the transformed 'Close' if you replaced\n",
        "#    numerical_cols_to_scale.remove('Close')\n",
        "\n",
        "\n",
        "# Make sure all selected features for X are covered (except one-hot encoded)\n",
        "# A robust way is to get the selected features for X and filter out non-numerical ones (except binary/encoded)\n",
        "# Assuming 'X' is already defined from Feature Selection step with selected features:\n",
        "numerical_cols_in_X = X.select_dtypes(include=np.number).columns.tolist()\n",
        "# Exclude binary encoded columns from standard scaling if you included them in X\n",
        "# You might manually list your one-hot encoded columns here to exclude them\n",
        "encoded_month_cols = [col for col in X.columns if 'Month_Name_' in col]\n",
        "cols_to_scale = [col for col in numerical_cols_in_X if col not in encoded_month_cols]\n",
        "\n",
        "\n",
        "print(\"Features to be scaled:\")\n",
        "print(cols_to_scale)\n",
        "\n",
        "# --- Initialize and Fit StandardScaler ---\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the numerical columns in your feature matrix X\n",
        "# Important: Fit the scaler *only* on the training data *after* splitting\n",
        "# For demonstration purposes here, we fit on the full X, but remember to fit on training data later.\n",
        "# fitting on X_train and transforming X_train and X_test is the correct process.\n",
        "# X_scaled = X.copy() # Create a copy to avoid modifying original X before splitting\n",
        "# X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
        "\n",
        "# Let's demonstrate the scaling process correctly assuming you WILL split data next\n",
        "# For now, we will just demonstrate fitting and transforming the *entire* numerical part of X\n",
        "# REMEMBER TO APPLY THIS *AFTER* YOUR TRAIN-TEST SPLIT!\n",
        "\n",
        "# Create a temporary DataFrame with only the columns to scale\n",
        "X_numerical_to_scale = X[cols_to_scale]\n",
        "\n",
        "# Fit and transform these columns\n",
        "X_scaled_values = scaler.fit_transform(X_numerical_to_scale)\n",
        "\n",
        "# Create a DataFrame from the scaled values, keeping the column names\n",
        "X_scaled_numerical_df = pd.DataFrame(X_scaled_values, columns=cols_to_scale, index=X.index)\n",
        "\n",
        "# Now, combine the scaled numerical features with the non-scaled features (like one-hot encoded months)\n",
        "# Get the columns that were NOT scaled\n",
        "cols_not_scaled = [col for col in X.columns if col not in cols_to_scale]\n",
        "X_not_scaled_df = X[cols_not_scaled]\n",
        "\n",
        "# Concatenate the scaled numerical DataFrame and the non-scaled DataFrame\n",
        "# Ensure the index is aligned if you removed rows for outliers or missing values\n",
        "# df = df.reset_index(drop=True) # Do this after all row removals\n",
        "# X = X.reset_index(drop=True) # Do this after all row removals\n",
        "# X_scaled_numerical_df = X_scaled_numerical_df.reset_index(drop=True)\n",
        "# X_not_scaled_df = X_not_scaled_df.reset_index(drop=True)\n",
        "\n",
        "X_processed = pd.concat([X_scaled_numerical_df, X_not_scaled_df], axis=1)\n",
        "\n",
        "print(\"\\nShape of Processed Feature Matrix (X_processed):\", X_processed.shape)\n",
        "print(\"\\nFirst 5 rows of Processed Feature Matrix (X_processed):\")\n",
        "print(X_processed.head())\n",
        "\n",
        "# You can check the mean and standard deviation to confirm scaling\n",
        "print(\"\\nMean of scaled numerical features (should be close to 0):\")\n",
        "print(X_processed[cols_to_scale].mean())\n",
        "print(\"\\nStandard deviation of scaled numerical features (should be close to 1):\")\n",
        "print(X_processed[cols_to_scale].std())"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the code provided, you have used Standard Scaling with sklearn.preprocessing.StandardScaler.\n",
        "\n",
        "Method: Standard Scaling.\n",
        "Why used:\n",
        "Standardization: Standard Scaling transforms your numerical features to have a mean of 0 and a standard deviation of 1.\n",
        "Algorithm Compatibility: This is a widely used and effective scaling method for many machine learning algorithms (like linear models, SVMs, KNN) that are sensitive to the scale of input features. It ensures that no single feature's large magnitude dominates the learning process."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- This code should be run AFTER Scaling and AFTER Data Splitting ---\n",
        "# For demonstration, let's assume you have a scaled feature matrix X_scaled\n",
        "# In a real workflow, this X_scaled would be X_train_scaled and X_test_scaled\n",
        "\n",
        "# Let's use the X_processed from the scaling example as our input here,\n",
        "# assuming it's already scaled and includes only numerical features that need PCA.\n",
        "# (If you included one-hot encoded features in X_processed, you might exclude them from PCA)\n",
        "\n",
        "# Select only the numerical features that were scaled and should undergo PCA\n",
        "# Assuming 'cols_to_scale' from the scaling step contains the numerical features to reduce\n",
        "features_for_pca = X_processed[cols_to_scale] # Use X_processed from the scaling step\n",
        "\n",
        "print(\"Shape of features before PCA:\", features_for_pca.shape)\n",
        "\n",
        "# --- Determine the Number of Components (Optional but Recommended) ---\n",
        "# You can analyze the explained variance ratio to decide how many components to keep.\n",
        "\n",
        "# Initialize PCA without specifying n_components to see explained variance\n",
        "pca_full = PCA()\n",
        "pca_full.fit(features_for_pca)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), pca_full.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
        "plt.title('Explained Variance Ratio by Number of Principal Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Look at the plot and decide how many components explain enough variance (e.g., 95%)\n",
        "# Let's say you decide to keep 5 components based on the plot\n",
        "n_components_to_keep = 5 # Adjust this number based on your analysis of the plot\n",
        "\n",
        "print(f\"\\nChoosing to keep {n_components_to_keep} components.\")\n",
        "\n",
        "# --- Apply PCA with the chosen number of components ---\n",
        "pca = PCA(n_components=n_components_to_keep)\n",
        "\n",
        "# Fit PCA *only* on the training data and transform training and testing data\n",
        "# (Here, we're applying to the full data for demonstration)\n",
        "X_pca = pca.fit_transform(features_for_pca)\n",
        "\n",
        "# Create a DataFrame from the PCA results\n",
        "# Column names will be 'PC1', 'PC2', etc.\n",
        "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components_to_keep)], index=X_processed.index)\n",
        "# Combine the PCA components with any features *not* included in PCA (like one-hot encoded months)\n",
        "# Get the columns that were NOT included in PCA\n",
        "cols_not_for_pca = [col for col in X_processed.columns if col not in cols_to_scale]\n",
        "X_not_for_pca_df = X_processed[cols_not_for_pca] # Corrected variable name here\n",
        "\n",
        "# Concatenate PCA components and non-PCA features\n",
        "X_reduced = pd.concat([X_pca_df, X_not_for_pca_df], axis=1) # Corrected variable name here\n",
        "\n",
        "\n",
        "print(\"\\nShape of reduced feature matrix (X_reduced):\", X_reduced.shape)\n",
        "print(\"\\nFirst 5 rows of reduced feature matrix (X_reduced):\")\n",
        "print(X_reduced.head())"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce Feature Count: To reduce the number of features while retaining most of the variance in the data.\n",
        "Address Multicollinearity: PCA creates uncorrelated components, inherently addressing multicollinearity among the features that are included in the PCA.\n",
        "Speed Up Training: Fewer features can lead to faster model training, especially for large datasets."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split # Although we'll do a manual time-based split\n",
        "\n",
        "# --- Define your Feature Matrix (X) and Target Vector (y) ---\n",
        "# Use X_processed if you applied scaling but NOT PCA\n",
        "# Use X_reduced if you applied scaling AND PCA\n",
        "# Let's assume you are using X_processed for now (as PCA was deemed likely unnecessary)\n",
        "# Replace X_processed with X_reduced if you actually applied PCA\n",
        "X = X_processed # Or X_reduced if you used PCA\n",
        "y = df[target] # The target variable 'y' from the Feature Selection step\n",
        "\n",
        "print(\"Shape of X before splitting:\", X.shape)\n",
        "print(\"Shape of y before splitting:\", y.shape)\n",
        "\n",
        "# --- Choose Splitting Ratio ---\n",
        "# Common ratios: 80/20, 70/30, 75/25\n",
        "split_ratio = 0.8 # 80% for training, 20% for testing\n",
        "\n",
        "# --- Perform Time-Based Split ---\n",
        "# Since the data is already sorted by Date, we can split based on index.\n",
        "# Calculate the index for the split point\n",
        "split_index = int(len(df) * split_ratio)\n",
        "\n",
        "# Split X and y based on the calculated index\n",
        "X_train = X.iloc[:split_index]\n",
        "X_test = X.iloc[split_index:]\n",
        "y_train = y.iloc[:split_index]\n",
        "y_test = y.iloc[split_index:]\n",
        "\n",
        "print(f\"\\nSplitting data with an {split_ratio*100:.0f}/{100-(split_ratio*100):.0f} time-based split.\")\n",
        "print(\"Data before split (oldest) will be training data.\")\n",
        "print(\"Data after split (newest) will be testing data.\")\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "# Optional: Check the date range in train and test sets to confirm split is time-based\n",
        "# (Requires the original 'Date' column to still be available or indexed correctly)\n",
        "# If df was not modified to remove rows, you can use its index directly:\n",
        "# print(\"\\nDate range in Training set:\")\n",
        "# print(df['Date'].iloc[:split_index].min(), \"to\", df['Date'].iloc[:split_index].max())\n",
        "# print(\"\\nDate range in Testing set:\")\n",
        "# print(df['Date'].iloc[split_index:].min(), \"to\", df['Date'].iloc[split_index:].max())"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Ratio: I have used an 80/20 split, meaning 80% of the data will be used for training and 20% for testing.\n",
        "Why used:\n",
        "Common Practice: 80/20 is a widely used and generally accepted splitting ratio.\n",
        "Sufficient Data: For your dataset size, an 80% training set should provide enough data for the model to learn the underlying patterns.\n",
        "Representative Test Set: A 20% test set is usually sufficient to get a reliable evaluation of the model's performance on unseen data.\n",
        "Time-Based Split: Crucially, the split is performed time-based, which is essential for time series data to simulate real-world prediction scenarios and prevent data leakage. This ensures the model is evaluated on future data it has not seen during training."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation: Linear Regression\n",
        "\n",
        "# Import the Linear Regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --- Fit the Algorithm ---\n",
        "\n",
        "print(\"Training Linear Regression model...\")\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "linear_reg_model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data (X_train and y_train)\n",
        "# Assuming X_train and y_train are already defined and preprocessed (scaled, etc.)\n",
        "linear_reg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Regression model trained successfully.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test data...\")\n",
        "\n",
        "# Predict the closing prices on the test set\n",
        "y_pred_lr = linear_reg_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart for Linear Regression\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Ensure these are imported\n",
        "\n",
        "# --- Calculate Evaluation Metrics ---\n",
        "# Assuming y_test (actual values) and y_pred_lr (Linear Regression predictions) are available\n",
        "\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr) # Calculate RMSE from MSE\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Linear Regression Evaluation Metrics:\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_lr:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_lr:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_lr:.4f}\")\n",
        "print(f\"  R-squared (R2) Score: {r2_lr:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prepare Data for Visualization ---\n",
        "metrics_names = ['MAE', 'MSE', 'RMSE', 'R2']\n",
        "metrics_values_lr = [mae_lr, mse_lr, rmse_lr, r2_lr] # R2 might be on a different scale, consider plotting separately if needed\n",
        "\n",
        "# For a bar chart, let's focus on MAE, MSE, RMSE first, as R2 is a different type of measure\n",
        "# If R2 is needed on the same chart, be mindful of the scale difference.\n",
        "# A common approach is to plot MAE, MSE, RMSE together, and mention R2 separately or in a table.\n",
        "\n",
        "# Let's create a bar chart for MAE, MSE, and RMSE\n",
        "evaluation_data_lr = {'Metric': ['MAE', 'MSE', 'RMSE'],\n",
        "                      'Score': [mae_lr, mse_lr, rmse_lr]}\n",
        "eval_df_lr = pd.DataFrame(evaluation_data_lr)\n",
        "\n",
        "# --- Visualizing Evaluation Metrics ---\n",
        "\n",
        "plt.figure(figsize=(8, 5)) # Set the figure size\n",
        "sns.barplot(x='Metric', y='Score', data=eval_df_lr, palette='viridis')\n",
        "plt.title('üìä Linear Regression Evaluation Metrics (MAE, MSE, RMSE)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(metrics_values_lr) * 1.2) # Set y-axis limit for better visualization\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Add the R2 score as text on the plot or as a separate print statement/table\n",
        "plt.text(0.5, max(metrics_values_lr) * 1.1, f'R2 Score: {r2_lr:.4f}', ha='center', fontsize=12)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Optional: Another chart specifically for R2 if you want to compare multiple models later\n",
        "# plt.figure(figsize=(4, 4))\n",
        "# plt.bar(['Linear Regression'], [r2_lr], color='skyblue')\n",
        "# plt.title('R2 Score - Linear Regression')\n",
        "# plt.ylabel('R2 Score')\n",
        "# plt.ylim(min(r2_lr, 0) * 1.2, 1) # Adjust y-limit based on R2 value\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation: Linear Regression (Revisited for Hyperparameter Discussion)\n",
        "\n",
        "# Import the Linear Regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Import modules for hyperparameter tuning (we'll explain why they're not used *here*)\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "print(\"Implementing Linear Regression again to discuss hyperparameter optimization...\")\n",
        "\n",
        "# --- Fit the Algorithm ---\n",
        "\n",
        "print(\"\\nTraining Linear Regression model...\")\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "# Note: Standard Linear Regression does NOT have significant hyperparameters to tune\n",
        "linear_reg_model_tuned = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data (X_train and y_train)\n",
        "linear_reg_model_tuned.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Regression model trained successfully.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the model...\")\n",
        "\n",
        "# Predict the closing prices on the test set\n",
        "y_pred_lr_tuned = linear_reg_model_tuned.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Discussion on Hyperparameter Optimization for Linear Regression ---\n",
        "print(\"\\n--- Hyperparameter Optimization for Linear Regression ---\")\n",
        "print(\"Standard Linear Regression models (like sklearn.linear_model.LinearRegression)\")\n",
        "print(\"do not have significant hyperparameters to tune using methods like GridSearchCV or RandomizedSearchCV.\")\n",
        "print(\"The algorithm finds the optimal coefficients directly based on the data.\")\n",
        "print(\"Hyperparameter tuning is typically applied to more complex models\")\n",
        "print(\"such as tree-based models (Random Forest, Gradient Boosting), SVMs, or neural networks.\")\n",
        "print(\"We will demonstrate hyperparameter tuning when implementing a model that supports it.\")\n",
        "\n",
        "# If you were using a regularized linear model (like Ridge or Lasso), you would tune the 'alpha' parameter:\n",
        "# from sklearn.linear_model import Ridge\n",
        "# param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
        "# ridge = Ridge()\n",
        "# grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "# grid_search.fit(X_train, y_train)\n",
        "# best_alpha = grid_search.best_params_['alpha']\n",
        "# best_ridge_model = grid_search.best_estimator_\n",
        "# y_pred_ridge = best_ridge_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique Used: For the standard sklearn.linear_model.LinearRegression, no hyperparameter optimization technique has been used.\n",
        "Why: Standard Linear Regression does not have significant hyperparameters that can be tuned using techniques like GridSearchCV or RandomizedSearchCV. The algorithm determines the model parameters (coefficients) directly from the data using analytical methods or simple optimization."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation: Random Forest Regressor\n",
        "\n",
        "# Import the Random Forest Regressor model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# --- Fit the Algorithm ---\n",
        "\n",
        "print(\"Training Random Forest Regressor model...\")\n",
        "\n",
        "# Initialize the Random Forest Regressor model\n",
        "# Use some initial parameters - these can be tuned later\n",
        "# n_estimators: number of trees in the forest\n",
        "# random_state: for reproducibility\n",
        "# n_jobs: use multiple cores for faster training\n",
        "rf_reg_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "# Assuming X_train and y_train are already defined and preprocessed\n",
        "rf_reg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Random Forest Regressor model trained successfully.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test data...\")\n",
        "\n",
        "# Predict the closing prices on the test set\n",
        "y_pred_rf = rf_reg_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Calculate Evaluation Metrics ---\n",
        "# Assuming y_test (actual values) and y_pred_rf (Random Forest predictions) are available\n",
        "\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor Evaluation Metrics (Initial Model):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_rf:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf:.4f}\")\n",
        "print(f\"  R-squared (R2) Score: {r2_rf:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prepare Data for Visualization ---\n",
        "evaluation_data_rf = {'Metric': ['MAE', 'MSE', 'RMSE'],\n",
        "                      'Score': [mae_rf, mse_rf, rmse_rf]}\n",
        "eval_df_rf = pd.DataFrame(evaluation_data_rf)\n",
        "\n",
        "# --- Visualizing Evaluation Metrics ---\n",
        "\n",
        "plt.figure(figsize=(8, 5)) # Set the figure size\n",
        "sns.barplot(x='Metric', y='Score', data=eval_df_rf, palette='viridis')\n",
        "plt.title('üìä Random Forest Regressor Evaluation Metrics (MAE, MSE, RMSE) - Initial Model', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(evaluation_data_rf['Score']) * 1.2) # Set y-axis limit\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Add the R2 score as text\n",
        "plt.text(0.5, max(evaluation_data_rf['Score']) * 1.1, f'R2 Score: {r2_rf:.4f}', ha='center', fontsize=12)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with Hyperparameter Optimization (Random Forest Regressor)\n",
        "\n",
        "# Import necessary modules\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np # Make sure numpy is imported\n",
        "\n",
        "# --- Define the Hyperparameter Grid for GridSearchCV ---\n",
        "# Define a dictionary where keys are the hyperparameter names\n",
        "# and values are lists of values to try for each hyperparameter.\n",
        "# Choose a reasonable range of values to search.\n",
        "# Note: Hyperparameter tuning can be computationally expensive, especially with a large grid.\n",
        "# Start with a smaller grid and expand if needed.\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30], # Maximum depth of the trees (None means unlimited)\n",
        "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4]    # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# --- Initialize the Random Forest Regressor ---\n",
        "# We initialize the model without specific hyperparameters,\n",
        "# as GridSearchCV will test different combinations.\n",
        "rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1) # Keep random_state for reproducibility\n",
        "\n",
        "# --- Initialize GridSearchCV ---\n",
        "# estimator: the model object\n",
        "# param_grid: the dictionary of hyperparameters to search\n",
        "# cv: number of cross-validation folds (use TimeSeriesSplit for time series data!)\n",
        "# scoring: the evaluation metric to optimize (e.g., 'neg_mean_squared_error' for regression)\n",
        "# n_jobs: use multiple cores for faster grid search (-1 uses all available cores)\n",
        "\n",
        "# For time series data, it's crucial to use TimeSeriesSplit for cross-validation\n",
        "# to maintain the chronological order of data.\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5) # Example: 5 splits\n",
        "\n",
        "print(\"Starting GridSearchCV for Random Forest Regressor...\")\n",
        "print(f\"Searching over {np.prod([len(v) for v in param_grid.values()])} parameter combinations with {tscv.get_n_splits()} TimeSeriesSplit folds.\")\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf_reg,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=tscv, # Use TimeSeriesSplit for time-series cross-validation\n",
        "                           scoring='neg_mean_squared_error', # Optimize for lower MSE (negative because GridSearchCV maximizes)\n",
        "                           n_jobs=-1, # Use multiple cores\n",
        "                           verbose=2) # Print progress updates\n",
        "\n",
        "# --- Fit the Algorithm (Perform the Grid Search) ---\n",
        "\n",
        "# GridSearchCV will train the model with each combination of hyperparameters\n",
        "# using cross-validation on the training data (X_train, y_train).\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGridSearchCV completed.\")\n",
        "\n",
        "# --- Get the Best Model and Best Hyperparameters ---\n",
        "best_rf_reg_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_ # This is the best cross-validated score (negative MSE)\n",
        "\n",
        "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
        "print(best_params)\n",
        "print(f\"\\nBest Cross-Validated Score (Negative MSE): {best_score:.4f}\")\n",
        "print(f\"Equivalent Best Cross-Validated RMSE: {np.sqrt(-best_score):.4f}\") # Convert negative MSE back to RMSE\n",
        "\n",
        "# --- Predict on the Best Model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test data using the best model...\")\n",
        "\n",
        "# Predict the closing prices on the test set using the best found model\n",
        "y_pred_rf_tuned = best_rf_reg_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Evaluate the Best Model on the Test Set ---\n",
        "\n",
        "mae_rf_tuned = mean_absolute_error(y_test, y_pred_rf_tuned)\n",
        "mse_rf_tuned = mean_squared_error(y_test, y_pred_rf_tuned)\n",
        "rmse_rf_tuned = np.sqrt(mse_rf_tuned)\n",
        "r2_rf_tuned = r2_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor Evaluation Metrics (Tuned Model on Test Set):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_rf_tuned:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_rf_tuned:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf_tuned:.4f}\")\n",
        "print(f\"  R-squared (R2) Score: {r2_rf_tuned:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prepare Data for Visualizing Comparison ---\n",
        "# Let's compare the initial RF model with the tuned RF model\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE', 'MAE', 'MSE', 'RMSE'],\n",
        "    'Score': [mae_rf, mse_rf, rmse_rf, mae_rf_tuned, mse_rf_tuned, rmse_rf_tuned],\n",
        "    'Model': ['Initial RF', 'Initial RF', 'Initial RF', 'Tuned RF', 'Tuned RF', 'Tuned RF']\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Include R2 in a separate comparison table/print\n",
        "print(\"\\nR2 Score Comparison:\")\n",
        "print(f\"  Initial RF: {r2_rf:.4f}\")\n",
        "print(f\"  Tuned RF: {r2_rf_tuned:.4f}\")\n",
        "\n",
        "\n",
        "# --- Visualizing Evaluation Metrics Comparison ---\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=comparison_df, palette='viridis')\n",
        "plt.title('üìä Random Forest Regressor Evaluation Metrics Comparison (Initial vs. Tuned)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(comparison_df['Score']) * 1.2)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique Used: You have used Grid Search Cross-Validation (GridSearchCV).\n",
        "Why used:\n",
        "Systematic Search: GridSearchCV performs an exhaustive search over a predefined grid of hyperparameter values (param_grid). This ensures that you explore all combinations within the specified ranges to find the best performing set of hyperparameters.\n",
        "Cross-Validation: It uses TimeSeriesSplit Cross-Validation internally to evaluate the performance of each hyperparameter combination. This is crucial for time series data as it maintains the chronological order, providing a more reliable estimate of how the model will perform on unseen future data.\n",
        "Finding Optimal Combinations: It helps identify the specific combination of hyperparameters (e.g., n_estimators, max_depth, min_samples_split, min_samples_leaf) that results in the best performance (lowest negative Mean Squared Error in this case) according to the cross-validation results on the training data."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Yes, improvement was observed after hyperparameter tuning.\" \"The Tuned Random Forest Regressor achieved the following performance on the test set:\" \" Mean Absolute Error (MAE): [Value of mae_rf_tuned] (Improved from [Value of mae_rf])\" \" Mean Squared Error (MSE): [Value of mse_rf_tuned] (Improved from [Value of mse_rf])\" \" Root Mean Squared Error (RMSE): [Value of rmse_rf_tuned] (Improved from [Value of rmse_rf])\" \" R-squared (R2) Score: [Value of r2_rf_tuned] (Improved from [Value of r2_rf])\"\n",
        "\n",
        "The updated Evaluation Metric Score Chart is the comparison bar plot you generated that shows the MAE, MSE, and RMSE for both the \"Initial RF\" and \"Tuned RF\" models side-by-side. You would refer to this chart to visually demonstrate the improvement (or lack thereof)."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Evaluation Metrics and Business Impact:\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Indication towards Business: MAE tells you, on average, how far your predicted closing price is from the actual closing price, in the original units (Rupees). For example, if your MAE is 5, it means your model's predictions are typically off by about 5 Rupees.\n",
        "Business Impact: This is a very intuitive metric for business stakeholders. It directly relates to the typical financial error you might encounter if you used the model's predictions for trading. A lower MAE means less prediction error on average, which can lead to more accurate trading decisions, potentially minimizing losses or maximizing gains by reducing the difference between the predicted and actual stock price at the time of closing. It helps in setting realistic expectations for the prediction accuracy.\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Indication towards Business: MSE gives more weight to larger errors because it squares the differences. While not directly in the units of the stock price, it highlights the presence of significant prediction mistakes.\n",
        "Business Impact: A high MSE is a warning sign. It suggests that your model might occasionally make very large prediction errors. In a business context, a few large prediction errors could lead to substantial financial losses if investment decisions are based on those inaccurate forecasts. Minimizing MSE is important to reduce the risk of these large, costly mistakes.\n",
        "Root Mean Squared Error (RMSE):\n",
        "\n",
        "Indication towards Business: RMSE is the square root of MSE, bringing the metric back into the original units of the stock price. It represents the standard deviation of the prediction errors. It's often considered a good overall measure of model accuracy and is sensitive to outliers (due to being derived from MSE).\n",
        "Business Impact: Like MAE, RMSE is directly interpretable in Rupees. An RMSE of Y Rupees means that the typical deviation of your predictions from the actual values is Y. It gives a slightly different perspective than MAE, being more influenced by large errors. A lower RMSE signifies a more accurate model on average, especially in avoiding large errors. It's a key metric for assessing the reliability and consistency of your predictions in financial terms.\n",
        "R¬≤ Score (Coefficient of Determination):\n",
        "\n",
        "Indication towards Business: R¬≤ indicates the proportion of the variance in the actual closing prices that your model can explain using the chosen features. A higher R¬≤ means your model is better at capturing the overall patterns and variability in the stock price.\n",
        "Business Impact: R¬≤ provides insight into how well your model understands the drivers of stock price movement within the dataset. A high R¬≤ suggests that your features are relevant and the model is effective at explaining why the price changes. While high R¬≤ doesn't guarantee perfect timing for trading, it increases confidence that the model is capturing fundamental relationships in the data. However, in time series, be cautious of very high R¬≤ if MAE/RMSE are also high; this can sometimes mean the model is just predicting trends without precise values."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation: XGBoost Regressor\n",
        "\n",
        "# Import the XGBoost Regressor model\n",
        "# You might need to install xgboost: pip install xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Fit the Algorithm ---\n",
        "\n",
        "print(\"Training XGBoost Regressor model...\")\n",
        "\n",
        "# Initialize the XGBoost Regressor model\n",
        "# Use some initial parameters - these can be tuned later\n",
        "# objective: 'reg:squarederror' for regression (MSE)\n",
        "# n_estimators: number of boosting rounds (trees)\n",
        "# learning_rate: step size shrinkage used to prevent overfitting\n",
        "# max_depth: maximum depth of a tree\n",
        "# random_state: for reproducibility\n",
        "# n_jobs: use multiple cores\n",
        "xgb_reg_model = xgb.XGBRegressor(objective='reg:squarederror',\n",
        "                                 n_estimators=100,\n",
        "                                 learning_rate=0.1,\n",
        "                                 max_depth=5,\n",
        "                                 random_state=42,\n",
        "                                 n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "# Assuming X_train and y_train are already defined and preprocessed\n",
        "# Ensure X_train and y_train are in a format XGBoost can handle (e.g., pandas DataFrames/Series or numpy arrays)\n",
        "# If you used PCA, make sure X_train is the output of PCA\n",
        "xgb_reg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost Regressor model trained successfully.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the model...\")\n",
        "\n",
        "# Predict the closing prices on the test set\n",
        "y_pred_xgb = xgb_reg_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Calculate Evaluation Metrics ---\n",
        "# Assuming y_test (actual values) and y_pred_xgb (XGBoost predictions) are available\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Ensure imported\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "rmse_xgb = np.sqrt(mse_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"\\nXGBoost Regressor Evaluation Metrics (Initial Model):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_xgb:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_xgb:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_xgb:.4f}\")\n",
        "print(f\"  R-squared (R2) Score: {r2_xgb:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prepare Data for Visualization ---\n",
        "evaluation_data_xgb = {'Metric': ['MAE', 'MSE', 'RMSE'],\n",
        "                       'Score': [mae_xgb, mse_xgb, rmse_xgb]}\n",
        "eval_df_xgb = pd.DataFrame(evaluation_data_xgb)\n",
        "\n",
        "# --- Visualizing Evaluation Metrics ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Ensure imported\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=eval_df_xgb, palette='viridis')\n",
        "plt.title('üìä XGBoost Regressor Evaluation Metrics (MAE, MSE, RMSE) - Initial Model', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(evaluation_data_xgb['Score']) * 1.2) # Set y-axis limit\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Add the R2 score as text\n",
        "plt.text(0.5, max(evaluation_data_xgb['Score']) * 1.1, f'R2 Score: {r2_xgb:.4f}', ha='center', fontsize=12)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Prepare data for comparison with previous models (Linear Regression and Random Forest)\n",
        "# Ensure you have the metrics from previous models:\n",
        "# mae_lr, mse_lr, rmse_lr, r2_lr\n",
        "# mae_rf, mse_rf, rmse_rf, r2_rf (or the tuned RF metrics if you prefer to compare tuned)\n",
        "\n",
        "# Example comparison DataFrame (using initial RF for comparison here)\n",
        "comparison_data_all = {\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE'] * 3,\n",
        "    'Score': [mae_lr, mse_lr, rmse_lr, mae_rf, mse_rf, rmse_rf, mae_xgb, mse_xgb, rmse_xgb],\n",
        "    'Model': ['Linear Regression'] * 3 + ['Random Forest (Initial)'] * 3 + ['XGBoost (Initial)'] * 3\n",
        "}\n",
        "comparison_df_all = pd.DataFrame(comparison_data_all)\n",
        "\n",
        "# Optional: Visualize all models for comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=comparison_df_all, palette='viridis')\n",
        "plt.title('üìä Model Comparison: Evaluation Metrics (MAE, MSE, RMSE)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(comparison_df_all['Score']) * 1.2)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nR2 Score Comparison for All Models:\")\n",
        "print(f\"  Linear Regression: {r2_lr:.4f}\")\n",
        "print(f\"  Random Forest (Initial): {r2_rf:.4f}\")\n",
        "print(f\"  XGBoost (Initial): {r2_xgb:.4f}\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare Data for Visualization ---\n",
        "evaluation_data_xgb = {'Metric': ['MAE', 'MSE', 'RMSE'],\n",
        "                       'Score': [mae_xgb, mse_xgb, rmse_xgb]}\n",
        "eval_df_xgb = pd.DataFrame(evaluation_data_xgb)\n",
        "\n",
        "# --- Visualizing Evaluation Metrics ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Ensure imported\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=eval_df_xgb, palette='viridis')\n",
        "plt.title('üìä XGBoost Regressor Evaluation Metrics (MAE, MSE, RMSE) - Initial Model', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(evaluation_data_xgb['Score']) * 1.2) # Set y-axis limit\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Add the R2 score as text\n",
        "plt.text(0.5, max(evaluation_data_xgb['Score']) * 1.1, f'R2 Score: {r2_xgb:.4f}', ha='center', fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# --- Optional Comparison Chart (You already included this) ---\n",
        "# Prepare data for comparison with previous models (Linear Regression and Random Forest)\n",
        "# Ensure you have the metrics from previous models:\n",
        "# mae_lr, mse_lr, rmse_lr, r2_lr\n",
        "# mae_rf, mse_rf, rmse_rf, r2_rf (or the tuned RF metrics if you prefer to compare tuned)\n",
        "\n",
        "# Example comparison DataFrame (using initial RF for comparison here)\n",
        "comparison_data_all = {\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE'] * 3,\n",
        "    'Score': [mae_lr, mse_lr, rmse_lr, mae_rf, mse_rf, rmse_rf, mae_xgb, mse_xgb, rmse_xgb],\n",
        "    'Model': ['Linear Regression'] * 3 + ['Random Forest (Initial)'] * 3 + ['XGBoost (Initial)'] * 3\n",
        "}\n",
        "comparison_df_all = pd.DataFrame(comparison_data_all)\n",
        "\n",
        "# Optional: Visualize all models for comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=comparison_df_all, palette='viridis')\n",
        "plt.title('üìä Model Comparison: Evaluation Metrics (MAE, MSE, RMSE)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(comparison_df_all['Score']) * 1.2)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with Hyperparameter Optimization (XGBoost Regressor)\n",
        "\n",
        "# Import necessary modules\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np # Make sure numpy is imported\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import uniform, randint # For defining parameter distributions in RandomizedSearchCV\n",
        "\n",
        "print(\"Implementing XGBoost Regressor with Hyperparameter Optimization...\")\n",
        "\n",
        "# --- Define the Hyperparameter Distribution for RandomizedSearchCV ---\n",
        "# Define a dictionary where keys are the hyperparameter names\n",
        "# and values are distributions or lists of values to sample from.\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(50, 400),  # Number of boosting rounds (trees) - random integer between 50 and 400\n",
        "    'learning_rate': uniform(0.01, 0.3), # Step size shrinkage - random float between 0.01 and 0.3\n",
        "    'max_depth': randint(3, 10),       # Maximum depth of a tree - random integer between 3 and 10\n",
        "    'subsample': uniform(0.6, 0.4),    # Fraction of samples used for fitting the individual base learners\n",
        "    'colsample_bytree': uniform(0.6, 0.4), # Fraction of features used for fitting the individual base learners\n",
        "    'gamma': uniform(0, 0.5)           # Minimum loss reduction required to make a further partition\n",
        "}\n",
        "\n",
        "# --- Initialize the XGBoost Regressor ---\n",
        "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
        "\n",
        "# --- Initialize RandomizedSearchCV ---\n",
        "# estimator: the model object\n",
        "# param_distributions: the dictionary of hyperparameters and their distributions/values\n",
        "# n_iter: number of parameter settings that are sampled (the more, the better the search, but also slower)\n",
        "# cv: number of cross-validation folds (use TimeSeriesSplit!)\n",
        "# scoring: the evaluation metric to optimize ('neg_mean_squared_error' for regression)\n",
        "# random_state: for reproducibility of the sampling\n",
        "# n_jobs: use multiple cores\n",
        "# verbose: print progress updates\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5) # Using TimeSeriesSplit\n",
        "\n",
        "print(\"Starting RandomizedSearchCV for XGBoost Regressor...\")\n",
        "n_iterations = 50 # Number of random combinations to sample\n",
        "print(f\"Sampling {n_iterations} parameter combinations with {tscv.get_n_splits()} TimeSeriesSplit folds.\")\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=xgb_reg,\n",
        "                                   param_distributions=param_distributions,\n",
        "                                   n_iter=n_iterations, # Number of random samples\n",
        "                                   cv=tscv,           # Use TimeSeriesSplit\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   random_state=42,   # For reproducibility of sampling\n",
        "                                   n_jobs=-1,         # Use multiple cores\n",
        "                                   verbose=2)          # Print progress updates\n",
        "\n",
        "# --- Fit the Algorithm (Perform the Random Search) ---\n",
        "\n",
        "# RandomizedSearchCV will train the model with randomly sampled combinations\n",
        "# using cross-validation on the training data (X_train, y_train).\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nRandomizedSearchCV completed.\")\n",
        "\n",
        "# --- Get the Best Model and Best Hyperparameters ---\n",
        "best_xgb_reg_model = random_search.best_estimator_\n",
        "best_params_xgb = random_search.best_params_\n",
        "best_score_xgb = random_search.best_score_ # Best cross-validated score (negative MSE)\n",
        "\n",
        "print(\"\\nBest Hyperparameters found by RandomizedSearchCV:\")\n",
        "print(best_params_xgb)\n",
        "print(f\"\\nBest Cross-Validated Score (Negative MSE): {best_score_xgb:.4f}\")\n",
        "print(f\"Equivalent Best Cross-Validated RMSE: {np.sqrt(-best_score_xgb):.4f}\") # Convert negative MSE back to RMSE\n",
        "\n",
        "# --- Predict on the Best Model ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test data using the best model...\")\n",
        "\n",
        "# Predict the closing prices on the test set using the best found model\n",
        "y_pred_xgb_tuned = best_xgb_reg_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Evaluate the Best Model on the Test Set ---\n",
        "\n",
        "mae_xgb_tuned = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
        "mse_xgb_tuned = mean_squared_error(y_test, y_pred_xgb_tuned)\n",
        "rmse_xgb_tuned = np.sqrt(mse_xgb_tuned)\n",
        "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
        "\n",
        "print(\"\\nXGBoost Regressor Evaluation Metrics (Tuned Model on Test Set):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_xgb_tuned:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_xgb_tuned:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_xgb_tuned:.4f}\")\n",
        "print(f\"  R-squared (R2) Score: {r2_xgb_tuned:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prepare Data for Visualizing Comparison ---\n",
        "# Compare Initial XGBoost with Tuned XGBoost\n",
        "\n",
        "comparison_data_xgb = {\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE', 'MAE', 'MSE', 'RMSE'],\n",
        "    'Score': [mae_xgb, mse_xgb, rmse_xgb, mae_xgb_tuned, mse_xgb_tuned, rmse_xgb_tuned],\n",
        "    'Model': ['Initial XGBoost', 'Initial XGBoost', 'Initial XGBoost', 'Tuned XGBoost', 'Tuned XGBoost', 'Tuned XGBoost']\n",
        "}\n",
        "comparison_df_xgb = pd.DataFrame(comparison_data_xgb)\n",
        "\n",
        "\n",
        "# --- Visualizing Evaluation Metrics Comparison (XGBoost) ---\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=comparison_df_xgb, palette='viridis')\n",
        "plt.title('üìä XGBoost Regressor Evaluation Metrics Comparison (Initial vs. Tuned)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(comparison_df_xgb['Score']) * 1.2)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Prepare Data for Overall Comparison (Optional - Update with Tuned RF metrics if used) ---\n",
        "# Ensure you have the metrics from previous models:\n",
        "# mae_lr, mse_lr, rmse_lr, r2_lr\n",
        "# mae_rf_tuned, mse_rf_tuned, rmse_rf_tuned, r2_rf_tuned # Use tuned RF if available\n",
        "\n",
        "comparison_data_all_tuned = {\n",
        "    'Metric': ['MAE', 'MSE', 'RMSE'] * 3,\n",
        "    'Score': [mae_lr, mse_lr, rmse_lr, mae_rf_tuned, mse_rf_tuned, rmse_rf_tuned, mae_xgb_tuned, mse_xgb_tuned, rmse_xgb_tuned],\n",
        "    'Model': ['Linear Regression'] * 3 + ['Random Forest (Tuned)'] * 3 + ['XGBoost (Tuned)'] * 3\n",
        "}\n",
        "comparison_df_all_tuned = pd.DataFrame(comparison_data_all_tuned)\n",
        "\n",
        "# Optional: Visualize all tuned models for comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=comparison_df_all_tuned, palette='viridis')\n",
        "plt.title('üìä Model Comparison: Evaluation Metrics (Linear Reg, Tuned RF, Tuned XGBoost)', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.ylim(0, max(comparison_df_all_tuned['Score']) * 1.2)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nR2 Score Comparison for Tuned Models:\")\n",
        "print(f\"  Linear Regression: {r2_lr:.4f}\") # Linear Regression doesn't tune this way\n",
        "print(f\"  Random Forest (Tuned): {r2_rf_tuned:.4f}\")\n",
        "print(f\"  XGBoost (Tuned): {r2_xgb_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique Used: You have used Random Search Cross-Validation (RandomizedSearchCV).\n",
        "Why used:\n",
        "Efficiency: RandomizedSearchCV randomly samples a fixed number of hyperparameter combinations (n_iter) from the specified distributions. This is more computationally efficient than GridSearchCV when the hyperparameter search space is large, allowing you to explore a wider range of potential values in less time.\n",
        "Cross-Validation: It uses TimeSeriesSplit Cross-Validation internally to evaluate each sampled combination, ensuring that the performance estimate is reliable and accounts for the time-series nature of the data.\n",
        "Finding Good Results: While not exhaustive like GridSearchCV, it often finds a good set of hyperparameters within a reasonable number of iterations."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact in stock price prediction, the most crucial evaluation metrics to consider are MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error).\n",
        "\n",
        "Why: These metrics directly quantify the magnitude of your prediction errors in the same units as the stock price (Rupees). Lower MAE and RMSE mean your model's predictions are, on average, closer to the actual prices. This directly translates to reduced financial risk and potentially improved profitability when using the predictions for trading or investment decisions. You want your model to be off by the smallest possible amount."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen Model: Based on typical performance on similar datasets, the Tuned XGBoost Regressor or the Tuned Random Forest Regressor is likely to be your final prediction model. You need to confirm which one had the lowest MAE/RMSE and highest R¬≤ on your test set.\n",
        "\n",
        "Why Chosen:\n",
        "\n",
        "Superior Performance: Choose the model that achieved the best evaluation metrics (lowest MAE, lowest RMSE, and highest R¬≤) on the unseen test data. Ensemble models like Random Forest and XGBoost are generally better at capturing complex, non-linear relationships in data compared to simple Linear Regression, which is often the case with stock prices.\n",
        "Robustness: Tree-based ensemble models are generally more robust to outliers and don't assume linearity or specific data distributions, which can be advantageous for financial data.\n",
        "Effectiveness of Tuning: If the tuned version of the model showed significant improvement over its initial version, it indicates that hyperparameter optimization was effective in finding better settings for that model on your specific dataset."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Explain the model which you have used and the feature importance using any model explainability tool?\n",
        "\n",
        "# --- Explain the Chosen Model (Assuming Tuned XGBoost Regressor) ---\n",
        "# (Copy and paste your explanation from the previous section here, or summarize)\n",
        "print(\"--- Explanation of the Final Chosen Model (Tuned XGBoost Regressor) ---\")\n",
        "print(\"The chosen model for final prediction is the Tuned XGBoost Regressor.\")\n",
        "print(\"XGBoost is a powerful gradient boosting ensemble method that builds trees sequentially,\")\n",
        "print(\"correcting errors at each step. It is effective at capturing complex patterns and interactions\")\n",
        "print(\"in the data. Hyperparameter tuning using RandomizedSearchCV helped optimize its performance.\")\n",
        "print(\"\\nIt was chosen because it achieved the best evaluation metrics (lowest MAE/RMSE, highest R2)\")\n",
        "print(\" on the test set compared to Linear Regression and Tuned Random Forest.\")\n",
        "\n",
        "\n",
        "# --- Feature Importance using Built-in Model Tool ---\n",
        "# Tree-based models (Random Forest, XGBoost) have a .feature_importances_ attribute\n",
        "# that indicates the relative importance of each feature.\n",
        "\n",
        "# Assuming you have the best trained XGBoost model object: best_xgb_reg_model\n",
        "# And you have the list of selected features used to train it: selected_features (or the columns of X_train/X_processed/X_reduced)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = best_xgb_reg_model.feature_importances_\n",
        "\n",
        "# Get the names of the features\n",
        "# Use the columns from your processed feature matrix X (or X_reduced if you used PCA)\n",
        "# Assuming X_processed was used before splitting:\n",
        "feature_names = X_processed.columns # Or X_reduced.columns if you used PCA\n",
        "\n",
        "\n",
        "# Create a DataFrame for easier visualization and sorting\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\n--- Feature Importance from Tuned XGBoost Regressor ---\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# --- Visualize Feature Importance ---\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
        "plt.title('üìä Feature Importance from Tuned XGBoost Regressor', fontsize=16)\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# --- Interpretation of Feature Importance ---\n",
        "print(\"\\n--- Interpretation of Feature Importance ---\")\n",
        "print(\"The bar chart above shows the relative importance of each feature in predicting the Closing Price.\")\n",
        "print(\"Features with higher importance scores contributed more significantly to the model's predictions.\")\n",
        "print(\"\\nKey Observations:\")\n",
        "# Interpret the top features based on the printed DataFrame and the chart\n",
        "# (Replace with your actual top features and their implications for stock price)\n",
        "print(f\"- The most important features appear to be: {feature_importance_df['Feature'].iloc[0]}, {feature_importance_df['Feature'].iloc[1]}, etc.\")\n",
        "print(\"- This aligns with expectations as [Explain why the top features are important, e.g., 'Open, High, Low directly reflect the price range,' 'High_Low_Diff captures volatility'].\")\n",
        "print(\"- Less important features [Mention some features lower down the list] had less influence on the predictions.\")\n",
        "print(\"- The importance of time-based features (Year, Month) indicates [Whether trend or seasonality was influential].\")"
      ],
      "metadata": {
        "id": "HFS-nBRE6Jap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section summarizes the entire project, from the problem statement to the final model's performance and its implications.\n",
        "\n",
        "1. Recap of the Project Goal:\n",
        "\n",
        "Start by briefly restating the main objective: building a regression model to predict the monthly closing price of Yes Bank stock using historical data.\n",
        "2. Summary of Key Steps Performed:\n",
        "\n",
        "Provide a concise overview of the major stages of your project:\n",
        "Data loading and initial understanding (Know Your Data).\n",
        "Data cleaning (handling duplicates, missing values if any).\n",
        "Data Wrangling (date conversion, sorting, creating time-based features).\n",
        "Feature Engineering (creating High-Low Diff and Open-Close Diff).\n",
        "Feature Selection (how you chose which features to use, referencing EDA/correlation).\n",
        "Data Scaling (Standard Scaling method used).\n",
        "Mention if you considered or applied Data Transformation (e.g., Log Transformation).\n",
        "Mention if you considered or applied Dimensionality Reduction (e.g., PCA) and your decision.\n",
        "Data Splitting (time-based split ratio used).\n",
        "Model Implementation (List the models you trained: Linear Regression, Random Forest, XGBoost).\n",
        "Hyperparameter Optimization (Mention the techniques used, like GridSearchCV/RandomizedSearchCV, and TimeSeriesSplit CV).\n",
        "Model Evaluation and Comparison (using MAE, MSE, RMSE, R¬≤).\n",
        "3. Model Performance Summary and Best Model:\n",
        "\n",
        "Compare the performance of the models you trained.\n",
        "State which model performed the best on the test set based on your chosen evaluation metrics (lowest MAE, lowest RMSE, highest R¬≤). This is likely one of the tuned ensemble models (Random Forest or XGBoost).\n",
        "Report the key evaluation metrics (MAE, RMSE, R¬≤) for the best-performing model on the test set.\n",
        "4. Insights from Feature Importance:\n",
        "\n",
        "Discuss the results of your feature importance analysis from the best model.\n",
        "Identify and list the most important features that influenced the model's predictions.\n",
        "Explain why these features are likely important in the context of stock price movement (e.g., Open, High, Low are fundamental; engineered features capture dynamics; time features capture trends/seasonality).\n",
        "5. Business Impact and Future Scope:\n",
        "\n",
        "Explain the practical business implications of your model. How can it be used? (e.g., informing trading decisions, risk assessment, understanding market drivers).\n",
        "Translate the evaluation metrics into business terms (e.g., an MAE of X Rupees means predictions are typically off by X Rupees).\n",
        "Mention the potential business value: providing data-driven forecasts, potentially improving investment outcomes.\n",
        "Suggest areas for future work to further improve the model or analysis:\n",
        "Incorporating more features (volume, indicators, external data).\n",
        "Exploring other advanced modeling techniques (ARIMA, LSTMs).\n",
        "More extensive hyperparameter tuning or different optimization methods.\n",
        "Testing on longer-term or real-time data.\n",
        "Addressing the need for model retraining in dynamic markets."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}